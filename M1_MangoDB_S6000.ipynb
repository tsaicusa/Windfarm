{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<p style=\"font-size:20px\"><b>Step 1: Data Preparation</b></p> \n",
    "Load data collected from DL_Cash project to MongoDb database.  This is the raw data set without any further preprocessing.\n",
    "Preprocessing is done in the next few steps.\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "oneDrive_root={}\n",
    "oneDrive_root[1]=\"C:\\\\Users\\\\Chihyang\\OneDrive for Business\\\\\"\n",
    "oneDrive_root[2]=\"C:\\\\Users\\\\Chihyang\\OneDrive for Business\\\\\"\n",
    "oneDrive_root[3]=\"C:\\\\Users\\\\tsaic\\\\OneDrive - State University of New York at New Paltz\\\\\"  # laptop\n",
    "WNF_lib = \".//WNF_lib//\"\n",
    "\n",
    "site=3   # the short or long business OneDrive directory name\n",
    "lib_dir=oneDrive_root[site]+'\\\\Prudentia\\\\Tsaipy'\n",
    "# append additional library path for this study\n",
    "sys.path.append(lib_dir)\n",
    "sys.path.append(WNF_lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "from dbconnect import mongodb as mdb\n",
    "import numpy as np\n",
    "import pprint\n",
    "from WFN_lib.cluster_classify import cluster_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"Windfarm_6000\"\n",
    "collection_structure = \"varStructures\"\n",
    "collection_data = \"Data_S6000\"\n",
    "collection_result = \"Results_S6000\"\n",
    "\n",
    "infile_name1 = \".//Results//data_sets//Complete_Features_Rating_Table.xlsx\"\n",
    "infile_name2 = \".//Results//data_sets//MinMax_data.xlsx\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<p style=\"font-size:20px\"><b>Connect to the database:</b></p> \n",
    "<ul>\n",
    "    <li>client: equivalent to <b>conn</b> in the return of a SQL connection request.</li>\n",
    "    <li>Check if the database with name <b>db_name</b> exists.  If yes, drop it to re-create a new one.</li>\n",
    "    <li>db: The reference to the database defined in \"<b>db_name</b>\"</li>\n",
    "    <li>The database is not actually created until data are put into it.</li>\n",
    "</ul>\n",
    "</div>        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing databases before adding the new one:  ['Windfarm_6000', 'admin', 'config', 'fin_clustering2002_2019', 'fin_clustering2005_2022', 'fin_clustering2019', 'local']\n",
      "db_name: Windfarm_6000\n",
      "Databases after adding the new one:  ['admin', 'config', 'fin_clustering2002_2019', 'fin_clustering2005_2022', 'fin_clustering2019', 'local']\n",
      "Database, Windfarm_6000, not found!!\n"
     ]
    }
   ],
   "source": [
    "## Create a new database, db_name, in MongoDB\n",
    "## If db_name exists, delete it first and then create a new one\n",
    "\n",
    "client = MongoClient('localhost', 27017)  # connect to the db engine\n",
    "dbnames = client.list_database_names()  # find all existing databases\n",
    "print(\"Existing databases before adding the new one: \",dbnames)\n",
    "# delete the database, db_name, if exists\n",
    "if db_name in dbnames:   \n",
    "    client.drop_database(db_name)\n",
    "else:\n",
    "    print(f\"Database, {db_name}, does not exist!!!\")\n",
    "\n",
    "# create a new database, db_name\n",
    "print(f\"db_name: {db_name}\")\n",
    "db = client[db_name]  # create a new database  \n",
    "\n",
    "## Check if the database is created.  At this point, it is not found since no data created in it yet.\n",
    "dbnames = client.list_database_names()  # find all existing databases\n",
    "print(\"Databases after adding the new one: \", dbnames)\n",
    "if db_name in dbnames:   # check if db_name is created\n",
    "    print(f\"Database, {db_name}, successfully created!!\")\n",
    "else:\n",
    "    print(f\"Database, {db_name}, not found!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style=\"font-size:20px\"><b>Create a database collection (table) for the \"structures\" of various collections in this step:</b></p> \n",
    "<ul>\n",
    "    <li><b>coll_struc</b>: The reference to the collection (equivalent to a SQL table) \"<b>collection_structure</b>\"</li>\n",
    "    <li>Note that a collection is not actually created until at least a document (record) is inserted to the collection.</li>\n",
    "    <li>Each row of this table store the field structure of a table.  Thus, the number of rows in this table equals the number of tables (collections) in the database</li>\n",
    "</ul>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection, varStructures, doesn't exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['_id_']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Drop the collection, collection_structure and recreate it anew.\n",
    "try:\n",
    "    db.validate_collection(collection_structure)  # Try to validate a collection   \n",
    "    db[collection_structure].drop()\n",
    "    print(f\"Collection (table), {collection_structure}, dropped\")\n",
    "except pymongo.errors.OperationFailure:  # If the collection doesn't exist\n",
    "        print(f\"Collection, {collection_structure}, doesn't exist\") \n",
    "     \n",
    "####### Create a new collections ##############################        \n",
    "coll_struc=db[collection_structure]   # collection (table) name\n",
    "\n",
    "####### Adding documents (rows) to the collection (table) ##########\n",
    "coll_struc.insert_one({\"_id\":\"V31_PLUS\",\"category\":\"extracted_data\",\"file_name\":\"Complete_Features_Rating_Table.xlsx\",\"directory\":\".//Results//data_sets//\",\n",
    "                           \"type\":\"31 feature variables\",\"in_collection\":\"Data_S6000\",\n",
    "                       \"desciption\":\"the 31 featured variables, 1-5 rating, and Random Forest classification probability (0,1) from Nguyen et al. (2021)\"})\n",
    "coll_struc.insert_one({\"_id\":\"V31_MinMax\", \"category\":\"scaled_data\",\"file_name\":\"MinMax_data.xlsx\",\"directory\":\".//Results//data_sets//\",\n",
    "                           \"type\":\"31 scaled feature variables\",\"in_collection\":\"Data_S6000\",\n",
    "                       \"desciption\":\"the 31 featured variables in V31_PLUS scaled to [0,1]\"})\n",
    "coll_struc.insert_one({\"_id\":\"S6000_wav\", \"category\":\"source data\",\"file_name\":\"Hn_nnnn.wav\",\"directory\":\".//wav//wav_files//\",\n",
    "                           \"type\":\"8192 rate\", \"in_collection\":\"None\",\"desciption\":\"The 6000 10 s. wav files with 8192 rate\"})\n",
    "coll_struc.insert_one({\"_id\":\"IOA_data\", \"category\":\"filtered_data\",\"file_name\":\"Hn_nnnn_bandFrq.npy\",\"directory\":\".//Windfarm_IOA//IOA_NPY_data_S6000_819//\",\n",
    "                           \"type\":\"819 rate\", \"in_collection\":\"Data_S6000\", \"description\":\"The 6000 10 s. octave band filtered and A-weighted data\"})\n",
    "#### Create an additional index on \"version\" \n",
    "####### This is in addition to the default/irremovable index _id.\n",
    "#result = coll_struc.create_index([('id', pymongo.ASCENDING)],unique=True)\n",
    "sorted(list(coll_struc.index_information()))  # show the indices        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test access to the newly created collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Windfarm_6000', 'admin', 'config', 'fin_clustering2002_2019', 'fin_clustering2005_2022', 'fin_clustering2019', 'local']\n",
      "res[_id]: V31_PLUS\n",
      "res[category]: extracted_data\n",
      "res[file_name]: Complete_Features_Rating_Table.xlsx\n",
      "res[directory]: .//Results//data_sets//\n",
      "res[type]: 31 feature variables\n",
      "res[in_collection]: Data_S6000\n",
      "res[desciption]: the 31 featured variables, 1-5 rating, and Random Forest classification probability (0,1) from Nguyen et al. (2021)\n"
     ]
    }
   ],
   "source": [
    "dbnames = client.list_database_names()  # find all existing databases\n",
    "print(dbnames)\n",
    "## Test retrieval of fields\n",
    "res = coll_struc.find_one({\"category\": \"extracted_data\"})\n",
    "# show elements in res\n",
    "for x in res:\n",
    "    ## get the content of the field, x\n",
    "    print(f\"res[{x}]: {res[x]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style=\"font-size:20px\"><b>Create a database collection (table) for the data:</b></p> There are three datasets in this data based on the same source wav files.  The Minmax scaler created to scale the data is saved under <b>.//trained_models</b>.\n",
    "<ul>\n",
    "    <li><b>v31</b>: The 31 feature variables per Nguyen</b>\"</li>\n",
    "    <li><b>v31_minmax</b>:The min_max scaled 31 variables into the range of (0,1).</li>\n",
    "    <li><b>IOA</b>:The octave band filtered, A-weighted data</li>\n",
    "</ul>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data source from an Excel file with three worksheets into pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert rows to collection_data collection\n",
    "- This process is done row-by-row using the insert_one() function\n",
    "- Each row obtains its data from the three sources, two from the dataframes and one from reading the npy file.\n",
    "- An additional index is created based on the stock __id__ field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection, Data_S6000, doesn't exist\n"
     ]
    }
   ],
   "source": [
    "### drop the dataset collection, collection_data\n",
    "try:\n",
    "    db.validate_collection(collection_data)  # Try to validate a collection   \n",
    "    db[collection_data].drop()\n",
    "    print(f\"Collection (table), {collection_data}, dropped\")\n",
    "except pymongo.errors.OperationFailure:  # If the collection doesn't exist\n",
    "        print(f\"Collection, {collection_data}, doesn't exist\") \n",
    "\n",
    "coll_data=db[collection_data]   # collection (table) name \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 35)\n",
      "        Site filename    Rating  Prob_AM_RdmFrst  spectralCentroid  \\\n",
      "H1_0001   H1   s1.wav  1.091934         0.174275         63.081251   \n",
      "H1_0002   H1   s2.wav  1.094161         0.019092         57.650009   \n",
      "H1_0003   H1   s3.wav  4.967208         0.648424         75.049887   \n",
      "\n",
      "         spectralCrest  spectralDecrease  spectralEntropy  spectralFlatness  \\\n",
      "H1_0001      28.079092        -92.466205         0.143241          0.010610   \n",
      "H1_0002      30.680069       -259.422066         0.067829          0.011763   \n",
      "H1_0003      25.424498        -22.843544         0.217764          0.008283   \n",
      "\n",
      "         spectralFlux  ...          PR   Fo  AMfactor       DAM  \\\n",
      "H1_0001  1.087143e-07  ...   69.138538  0.9  0.239249  1.339453   \n",
      "H1_0002  4.547078e-05  ...    7.843309  0.8  0.111174  1.217609   \n",
      "H1_0003  3.236673e-06  ...  104.108914  0.7  0.827716  7.589734   \n",
      "\n",
      "         peakloc_unweightedSPL       L63      L125      L250      L500  \\\n",
      "H1_0001                    0.2  3.403435  2.685078  3.967624  1.748563   \n",
      "H1_0002                    0.5  6.255368  6.074805  3.424099  2.094934   \n",
      "H1_0003                    2.8  7.478385  6.192284  6.185724  5.644156   \n",
      "\n",
      "             L1000  \n",
      "H1_0001   1.427153  \n",
      "H1_0002   1.888449  \n",
      "H1_0003  13.776354  \n",
      "\n",
      "[3 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "### For the 31 feature  ########################################\n",
    "## Retrieve the source data from the Excel file, infile_name1\n",
    "df_v31=pd.read_excel(infile_name1,header=0,index_col=0)\n",
    "print(df_v31.shape)\n",
    "print(df_v31.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 31)\n",
      "         spectralCentroid  spectralCrest  spectralDecrease  spectralEntropy  \\\n",
      "H1_0001          0.012519       0.871260          0.999324         0.142351   \n",
      "H1_0002          0.008121       0.957792          0.998104         0.065464   \n",
      "H1_0003          0.022211       0.782944          0.999833         0.218333   \n",
      "\n",
      "         spectralFlatness  spectralFlux  spectralKurtosis  \\\n",
      "H1_0001          0.010966      0.001331          0.004294   \n",
      "H1_0002          0.012185      0.557133          0.002559   \n",
      "H1_0003          0.008507      0.039656          0.000540   \n",
      "\n",
      "         spectralRolloffPoint  spectralSkewness  spectralSlope  ...        PR  \\\n",
      "H1_0001              0.021432          0.078659       0.997918  ...  0.064817   \n",
      "H1_0002              0.007925          0.068858       0.622292  ...  0.007353   \n",
      "H1_0003              0.040809          0.027030       0.967046  ...  0.097601   \n",
      "\n",
      "               Fo  AMfactor       DAM  peakloc_unweightedSPL       L63  \\\n",
      "H1_0001  1.000000  0.047060  0.015886               0.000000  0.046839   \n",
      "H1_0002  0.888889  0.017838  0.013268               0.065217  0.099330   \n",
      "H1_0003  0.777778  0.181323  0.150206               0.565217  0.121841   \n",
      "\n",
      "             L125      L250      L500     L1000  \n",
      "H1_0001  0.034494  0.050674  0.015160  0.008636  \n",
      "H1_0002  0.100103  0.039823  0.022587  0.018756  \n",
      "H1_0003  0.102376  0.094960  0.098695  0.279557  \n",
      "\n",
      "[3 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "## read in the Minmax scaled data processed earlier for verification purpose\n",
    "## We need scale the data to [0,1] again in order to obtain and save the scaler for later use.\n",
    "df_v31minmax_veri=pd.read_excel(infile_name2,header=0,index_col=0)\n",
    "print(df_v31minmax_veri.shape)\n",
    "print(df_v31minmax_veri.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 31)\n",
      "         spectralCentroid  spectralCrest  spectralDecrease  spectralEntropy  \\\n",
      "H1_0001          0.012519       0.871260          0.999324         0.142351   \n",
      "H1_0002          0.008121       0.957792          0.998104         0.065464   \n",
      "H1_0003          0.022211       0.782944          0.999833         0.218333   \n",
      "\n",
      "         spectralFlatness  spectralFlux  spectralKurtosis  \\\n",
      "H1_0001          0.010966      0.001331          0.004294   \n",
      "H1_0002          0.012185      0.557133          0.002559   \n",
      "H1_0003          0.008507      0.039656          0.000540   \n",
      "\n",
      "         spectralRolloffPoint  spectralSkewness  spectralSlope  ...        PR  \\\n",
      "H1_0001              0.021432          0.078659       0.997918  ...  0.064817   \n",
      "H1_0002              0.007925          0.068858       0.622292  ...  0.007353   \n",
      "H1_0003              0.040809          0.027030       0.967046  ...  0.097601   \n",
      "\n",
      "               Fo  AMfactor       DAM  peakloc_unweightedSPL       L63  \\\n",
      "H1_0001  1.000000  0.047060  0.015886               0.000000  0.046839   \n",
      "H1_0002  0.888889  0.017838  0.013268               0.065217  0.099330   \n",
      "H1_0003  0.777778  0.181323  0.150206               0.565217  0.121841   \n",
      "\n",
      "             L125      L250      L500     L1000  \n",
      "H1_0001  0.034494  0.050674  0.015160  0.008636  \n",
      "H1_0002  0.100103  0.039823  0.022587  0.018756  \n",
      "H1_0003  0.102376  0.094960  0.098695  0.279557  \n",
      "\n",
      "[3 rows x 31 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.//trained_models//Minmax_scaler_v31_S6000.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Minmax scaling the 31 feasure variables: columns of df_v31[4:35]\n",
    "## The first 4 columns are not scaled. \n",
    "## Check if the first three rows of the scaled data are the same as those from df_v31minmax_veri\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_v31minmax = scaler.fit_transform(df_v31.iloc[:,4:35])\n",
    "df_v31minmax = pd.DataFrame(df_v31minmax, columns=df_v31.columns[4:35], index=df_v31.index)\n",
    "print(df_v31minmax.shape)\n",
    "## Check the first three rows of the scaled data with those from df_v31minmax_veri\n",
    "print(df_v31minmax.head(3))\n",
    "## Save the scaler to be used for scaling new data\n",
    "import joblib\n",
    "joblib.dump(scaler, './/trained_models//Minmax_scaler_v31_S6000.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store data in the collection Data_S6000\n",
    "- First three columns: _id, site, file_name\n",
    "- v31: 31 columns for the original 31 feature variables\n",
    "- v31_minmax: minmax scaled v31 variables\n",
    "- IOA: four bands 25-100Hz,50-200Hz, 100-400Hz, 200-800Hz, each with 100 values (No threshold, 819 Hz frame version), 100 values for each sample in each band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For the 31 feature variables and the 31 scaled feature variables ########################################\n",
    "v31_keys=list(df_v31minmax.columns)   # variable names (or column names in the collection), v31 and v31_minmax share the same keys\n",
    "### For reading and organizing IOA data ###################################################################\n",
    "dir_npy='.//Windfarm_IOA//IOA_NPY_data_S6000_819//'  # A-weighted 819 frame rate data\n",
    "bands = ['025_100Hz','050_200Hz', '100_400Hz', '200_800Hz']\n",
    "post_fix=['_025_100Hz.npy','_050_200Hz.npy', '_100_400Hz.npy', '_200_800Hz.npy']\n",
    "IOA_keys = ['v'+str(i).zfill(3) for i in range(100)]   # for the 100 values extracted from 819 rate time series data\n",
    "#############################################################################################################\n",
    "#df_v31.head(3)\n",
    "for i, idx in enumerate(df_v31.index):\n",
    "    pymongo_data_dict=dict()\n",
    "    row = df_v31.loc[idx]    # v31\n",
    "    row_minmax = df_v31minmax.loc[idx]   #v31_minmax\n",
    "    \n",
    "    #print(f\"{i}: {idx}\")\n",
    "\n",
    "    ### file information\n",
    "    pymongo_data_dict[\"_id\"]=idx\n",
    "    pymongo_data_dict[\"site\"]=row[\"Site\"]\n",
    "    pymongo_data_dict[\"file_name\"]=row[\"filename\"]\n",
    "\n",
    "    pymongo_data_dict[\"v31\"]=dict(); \n",
    "    pymongo_data_dict[\"v31_minmax\"]=dict()\n",
    "    pymongo_data_dict[\"IOA\"]=dict()\n",
    "\n",
    "    for key in v31_keys:  #  collect the values of the V31 and V31_minmax variables\n",
    "        pymongo_data_dict[\"v31\"][key]=row[key]          \n",
    "        pymongo_data_dict[\"v31_minmax\"][key]=row_minmax[key]\n",
    "        \n",
    "    ## for IOA data, collect the values of the 4 bands for each file\n",
    "    for (band, pos) in zip(bands, post_fix):\n",
    "        pymongo_data_dict['IOA'][band]=list(np.load(dir_npy+idx+pos))\n",
    "\n",
    "\n",
    "    ## insert the document into the collection\n",
    "    coll_data.insert_one(pymongo_data_dict)\n",
    "    #if i>3: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Create a collection for analysis result</b>\n",
    "<ol>\n",
    "<li>scores: The rating and Random forest result from Nguyen 2021 based on 31 summary variables.</li>\n",
    "<li>clusters: clustering results based the 31 variables, K=3, K=4, K=5, K=6</li>\n",
    "<li>freqency bands: from IOA rating (AMWG)\n",
    "<li>classes, probs: the top five classes and corresponding probabilities from YAMNET\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Result database content\n",
    "In the update statement, the last 2 True/False fields specifies the upsert and multi flags.\n",
    "\n",
    "- Upsert flag: If set to true, creates a new document when no document matches the query criteria.\n",
    "\n",
    "- Multi flag: If set to true, updates multiple documents that meet the query criteria. If set to false, updates one document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection, Results_S6000, doesn't exist\n"
     ]
    }
   ],
   "source": [
    "### drop the result set collection, collection_result\n",
    "try:\n",
    "    db.validate_collection(collection_result)  # Try to validate a collection   \n",
    "    db[collection_result].drop()\n",
    "    print(f\"Collection (table), {collection_result}, dropped\")\n",
    "except pymongo.errors.OperationFailure:  # If the collection doesn't exist\n",
    "        print(f\"Collection, {collection_result}, doesn't exist\") \n",
    "\n",
    "coll_result=db[collection_result]   # collection (table) name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result 1: From the nguyen's paper\n",
    "## These two scores are also in the collection, collection_data (Data_S6000)\n",
    "df_v31.head(3)  \n",
    "scores = [\"Rating\",\"Prob_AM_RdmFrst\"]\n",
    "for i, idx in enumerate(df_v31.index):\n",
    "    pymongo_result_dict=dict()\n",
    "    #print(f\"{i}: {idx}\")\n",
    "    row = df_v31.loc[idx]\n",
    "\n",
    "    pymongo_result_dict[\"_id\"]=idx\n",
    "    pymongo_result_dict[\"site\"]=row[\"Site\"]\n",
    "    #pymongo_result_dict[\"file_name\"]=row[\"filename\"]\n",
    "\n",
    "    pymongo_result_dict[\"scores\"]=dict()\n",
    "    ###############################\n",
    "    for s in scores:\n",
    "        pymongo_result_dict[\"scores\"][s]=row[s]\n",
    "    ## insert the document into the collection\n",
    "    coll_result.insert_one(pymongo_result_dict)\n",
    "    #if i>3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         K=2  K=3  K=4  K=5  K=6  K=7  K=8  K=9  K=10\n",
      "H1_0001    0    2    3    0    5    6    0    0     3\n",
      "H1_0002    1    0    1    4    2    5    3    3     7\n",
      "H1_0003    0    1    3    0    5    6    0    1     2\n",
      "(6000, 9)\n",
      "         K=2  K=3  K=4  K=5  K=6  K=7  K=8  K=9  K=10\n",
      "H1_0001    0    2    3    0    5    6    0    0     3\n",
      "H1_0002    1    0    1    4    2    5    3    3     7\n",
      "H1_0003    0    1    3    0    5    6    0    1     2\n",
      "(6000, 9)\n"
     ]
    }
   ],
   "source": [
    "## Result 2: Clustering result by Kmeans with K=2, 3, .., 10 and save the clustering models\n",
    "### Cluster the 6000 samples into 2-10 clusters\n",
    "df_S6000_cluster, _ =cluster_analysis(df_v31minmax, [2,3,4,5,6,7,8,9,10], random_state=42, scaler_type=None,n_init=10, max_iter=300, tol=0.0001,\n",
    "                                 save_model_dir='.//trained_models//', model_file_name='Kmeans_S6000_v31_MinMax_K=')\n",
    "print(df_S6000_cluster.head(3))\n",
    "print(df_S6000_cluster.shape)\n",
    "\n",
    "## Double check with the saved clustering result\n",
    "infile_name3 = \".//Results//cluster_allotment//Cluster_S6000_MinMax_result.xlsx\"\n",
    "df_cluster=pd.read_excel(infile_name3,header=0,index_col=0)\n",
    "print(df_cluster.head(3))\n",
    "print(df_cluster.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Result 2: Clustering result\n",
    "infile_name3 = \".//Results//cluster_allotment//Cluster_S6000_MinMax_result.xlsx\"\n",
    "df_cluster=pd.read_excel(infile_name3,header=0,index_col=0)\n",
    "df_cluster.head(3)\n",
    "\n",
    "pymongo_cluster_dict=dict()\n",
    "for i, idx in enumerate(df_S6000_cluster.index):\n",
    "    pymongo_cluster_dict[idx]=dict()\n",
    "    #print(f\"{i}: {idx}\")\n",
    "    row = df_cluster.loc[idx]\n",
    "\n",
    "    ## mongodb does not allow np.int64 as a value, chagne it to int\n",
    "    pymongo_cluster_dict[idx][\"K=2\"]=int(row[\"K=2\"])\n",
    "    pymongo_cluster_dict[idx][\"K=3\"]=int(row[\"K=3\"])\n",
    "    pymongo_cluster_dict[idx][\"K=4\"]=int(row[\"K=4\"])\n",
    "    pymongo_cluster_dict[idx][\"K=5\"]=int(row[\"K=5\"])\n",
    "    pymongo_cluster_dict[idx][\"K=6\"]=int(row[\"K=6\"])\n",
    "    pymongo_cluster_dict[idx][\"K=7\"]=int(row[\"K=7\"])\n",
    "    pymongo_cluster_dict[idx][\"K=8\"]=int(row[\"K=8\"])\n",
    "    pymongo_cluster_dict[idx][\"K=9\"]=int(row[\"K=9\"])\n",
    "    pymongo_cluster_dict[idx][\"K=10\"]=int(row[\"K=10\"])\n",
    "    \n",
    "\n",
    "    ## Append the new result for idx to the document \n",
    "    #print(pymongo_cluster_dict[idx])\n",
    "    coll_result.update_one({\"_id\":idx },{'$set' : {\"clusters\": pymongo_cluster_dict[idx]}}, False, False)\n",
    "    #if i>3: break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', '025_100Hz', '050_200Hz', '100_400Hz', '200_800Hz']\n"
     ]
    }
   ],
   "source": [
    "## Result 3: from IOA results\n",
    "infile_name4=\".//Windfarm_IOA//IOA_Result_819_S6000//All6000_IOA_AW_819_NoThreshold.xlsx\"\n",
    "xls = pd.ExcelFile(infile_name4)\n",
    "sheet_names = xls.sheet_names  # see all sheet names\n",
    "print(sheet_names)\n",
    "keys=None\n",
    "\n",
    "### Get the data from each sheet\n",
    "df_bands = dict()\n",
    "for sht in sheet_names[1:]:\n",
    "    df_bands[sht]=pd.read_excel(infile_name4,sheet_name=sht,header=0,index_col=0)\n",
    "    \n",
    "    #print(df_bands[sht].head(3))\n",
    "    if sht==sheet_names[1]:\n",
    "        indices = df_bands[sht].index  # get the index names of the first sheet, since they are the same for all sheets\n",
    "    \n",
    "keys = ['prominence','L5-L95']   # get the column names of the two IOA values to extract\n",
    "\n",
    "############################################################\n",
    "\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    pymongo_sht_dict=dict()\n",
    "    for sht in sheet_names[1:]:\n",
    "        pymongo_sht_dict[sht]=dict()\n",
    "        ## find the index of the row in coll_result, with the same id\n",
    "        row = df_bands[sht].loc[idx]\n",
    "        #print(f\"sht = {sht}, idx = {idx}\")\n",
    "\n",
    "        for key in keys:  # skip the first first column, 'band', which is the parent key\n",
    "            pymongo_sht_dict[sht][key]=row[key]\n",
    "        ## append the document to the end of the document with the same id if the document with the same id exists, otherwise show error message\n",
    "\n",
    "    #print(pymongo_sht_dict)\n",
    "    coll_result.update_one({\"_id\":idx },{'$set' : {\"IOA_rating\":pymongo_sht_dict}}, False, False)\n",
    "\n",
    "    #if i>3: break\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['top_1', 'top_2', 'top_3', 'top_4', 'top_5'], dtype='object') Index(['prob_1', 'prob_2', 'prob_3', 'prob_4', 'prob_5'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsaic\\AppData\\Local\\Temp\\ipykernel_11748\\4178203803.py:32: DeprecationWarning: update is deprecated. Use replace_one, update_one or update_many instead.\n",
      "  coll_result.update({\"_id\":idx },{'$set' : {\"YAMnet\": pymongo_YAMNET_dict}}, True, True)\n"
     ]
    }
   ],
   "source": [
    "## Result 4: YAMNET results\n",
    "infile_name5=\".//Results//classification//YAMNET_cls_results.xlsx\"\n",
    "df_H1=pd.read_excel(infile_name5,sheet_name=\"H1_cls\",header=0,index_col=0)\n",
    "df_H2=pd.read_excel(infile_name5,sheet_name=\"H2_cls\",header=0,index_col=0)\n",
    "### concatenate the two dataframes\n",
    "df_H=pd.concat([df_H1,df_H2],axis=0)\n",
    "classes=df_H.columns[:5]  # the first 5 columns are the class names\n",
    "probs=df_H.columns[5:]  # the rest of the columns are the probabilities\n",
    "print(classes, probs)\n",
    "#############################################################################\n",
    "#pymongo_class_dict=dict()\n",
    "#pymongo_prob_dict=dict()\n",
    "\n",
    "for i, idx in enumerate(df_H.index):\n",
    "    pymongo_YAMNET_dict=dict()\n",
    "    pymongo_YAMNET_dict['class']=dict()\n",
    "    pymongo_YAMNET_dict['prob']=dict()\n",
    "    ## find the index of the row in coll_result, with the same id\n",
    "    #pymongo_YAMNET_dict['class'][idx]=dict()\n",
    "    #pymongo_YAMNET_dict['prob'][idx]=dict()\n",
    "    row = df_H.loc[idx]\n",
    "\n",
    "    for s in classes:\n",
    "        pymongo_YAMNET_dict['class'][s]=row[s]\n",
    "    for s in probs:\n",
    "        pymongo_YAMNET_dict['prob'][s]=row[s]\n",
    "\n",
    "    #print(pymongo_class_dict[idx])\n",
    "    #print(pymongo_prob_dict[idx])\n",
    "    ## append the document to the end of the document with the same id if the document with the same id exists, \n",
    "    #coll_result.update({\"id\":idx },{'$set' : {\"classes\": pymongo_class_dict[idx], \"probs\":pymongo_prob_dict[idx]}}, True, True)\n",
    "    coll_result.update({\"_id\":idx },{'$set' : {\"YAMnet\": pymongo_YAMNET_dict}}, True, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index for collection varStructures: {'_id_': {'v': 2, 'key': [('_id', 1)]}}\n",
      "Index for collection Data_S6000: {'_id_': {'v': 2, 'key': [('_id', 1)]}}\n",
      "Index for collection Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'Windfarm_6000'), 'Results_S6000'): {'_id_': {'v': 2, 'key': [('_id', 1)]}}\n"
     ]
    }
   ],
   "source": [
    "## find the index column of the collection, collection_data\n",
    "print(f\"Index for collection {collection_structure}: {coll_struc.index_information()}\")\n",
    "print(f\"Index for collection {collection_data}: {coll_data.index_information()}\")\n",
    "print(f\"Index for collection {coll_result}: {coll_result.index_information()}\")\n",
    "## To create an additional index, use the following command\n",
    "#result = coll_data.create_index([('column_name', pymongo.ASCENDING)],unique=True)\n",
    "#where column_name is the name of the column to be indexed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<p style=\"font-size:20px\"><b>What have you achieved:</b></p> \n",
    "At this point, we have created the following.\n",
    "<ul>\n",
    "    <li>Collection <b>varStructures</b>: a collection that lists the source file category, name, location etc. with a brief descriptionile.</li>\n",
    "    <li>Collection <b>Data_S6000</b>: Source data pertaining to each of the 6000 files,  including the 31 feature variables, their min_max scaled version, and IOA data.</li>\n",
    "    <li>Collection <b>Results_S6000</b>: Analysis results pertaining to each of the 6000 files,  including (1) the original two scoring variables, rating and random forest probability from Nguyen, 2021, (2) clustering analysis result for K=3,4,5,6, (3) IOA Rating, (4) YAMNET results (five first pick categories and their associated probabilities.</li>\n",
    "</ul>\n",
    "</div>   "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce7598402296efdb2a2a3ae171a6e3871c2d8faeca73f35326143a0f1e59ad6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
