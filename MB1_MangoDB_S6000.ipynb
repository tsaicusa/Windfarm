{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<p style=\"font-size:20px\"><b>Section 1: Data Preparation for S6000: </b></p> \n",
    "Load Windfarm Data from H1 and H2 into a mongoDB database, Data_S6000.  It includes\n",
    "</ul>\n",
    "<li>Raw data of the 31 feature variables (v31)</li>\n",
    "<li>Minmax scaled v31 data.</li>\n",
    "<li>Octave band filtered data (Butterworth) going through IOA filter with 819 Hz frame and A weighting.  There are 100 data points for each band.</li>\n",
    "</ul>\n",
    "\n",
    "All trained models using <b>sklearn modules</b> are saved, including all <b>min_max scalers</b> and <b>KMeans clustering</b> models.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "oneDrive_root={}\n",
    "oneDrive_root[1]=\"C:\\\\Users\\\\Chihyang\\OneDrive for Business\\\\\"\n",
    "oneDrive_root[2]=\"C:\\\\Users\\\\Chihyang\\OneDrive for Business\\\\\"\n",
    "oneDrive_root[3]=\"C:\\\\Users\\\\tsaic\\\\OneDrive - State University of New York at New Paltz\\\\\"  # laptop\n",
    "WNF_lib = \".//WNF_lib//\"\n",
    "\n",
    "site=3   # the short or long business OneDrive directory name\n",
    "lib_dir=oneDrive_root[site]+'\\\\Prudentia\\\\Tsaipy'\n",
    "# append additional library path for this study\n",
    "sys.path.append(lib_dir)\n",
    "sys.path.append(WNF_lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "from dbconnect import mongodb as mdb\n",
    "import numpy as np\n",
    "import pprint\n",
    "from WFN_lib.cluster_classify import cluster_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"Windfarm_S6000\"\n",
    "collection_data = \"S6000_Data\"\n",
    "infile_name1 = \".//Results//data_sets//Complete_Features_Rating_Table.xlsx\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style=\"font-size:20px\"><b>Section 1.1: Connect to the database:</b></p> \n",
    "<ul>\n",
    "    <li>client: equivalent to <b>conn</b> in the return of a SQL connection request.</li>\n",
    "    <li>Check if the database with name <b>db_name</b> exists.  If yes, drop it to re-create a new one.</li>\n",
    "    <li>db: The reference to the database defined in \"<b>db_name</b>\"</li>\n",
    "    <li>The database is not actually created until data are put into it.</li>\n",
    "</ul>\n",
    "</div>        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing databases before adding the new one:  ['Windfarm_6000', 'Windfarm_S6000', 'admin', 'config', 'fin_clustering2002_2019', 'fin_clustering2005_2022', 'fin_clustering2019', 'local']\n",
      "db_name: Windfarm_S6000\n",
      "Databases after adding the new one:  ['Windfarm_6000', 'admin', 'config', 'fin_clustering2002_2019', 'fin_clustering2005_2022', 'fin_clustering2019', 'local']\n",
      "Database, Windfarm_S6000, not found!!\n"
     ]
    }
   ],
   "source": [
    "## Create a new database, db_name, in MongoDB\n",
    "## If db_name exists, delete it first and then create a new one\n",
    "\n",
    "client = MongoClient('localhost', 27017)  # connect to the db engine\n",
    "dbnames = client.list_database_names()  # find all existing databases\n",
    "print(\"Existing databases before adding the new one: \",dbnames)\n",
    "# delete the database, db_name, if exists\n",
    "if db_name in dbnames:   \n",
    "    client.drop_database(db_name)\n",
    "else:\n",
    "    print(f\"Database, {db_name}, does not exist!!!\")\n",
    "\n",
    "# create a new database, db_name\n",
    "print(f\"db_name: {db_name}\")\n",
    "db = client[db_name]  # create a new database  \n",
    "\n",
    "## Check if the database is created.  At this point, it is not found since no data created in it yet.\n",
    "dbnames = client.list_database_names()  # find all existing databases\n",
    "print(\"Databases after adding the new one: \", dbnames)\n",
    "if db_name in dbnames:   # check if db_name is created\n",
    "    print(f\"Database, {db_name}, successfully created!!\")\n",
    "else:\n",
    "    print(f\"Database, {db_name}, not found!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style=\"font-size:20px\"><b>Section 1.2: Create a database collection (table) for the data:</b></p> There are three datasets in this data based on the same source wav files.  The Minmax scaler created to scale the data is saved under <b>.//trained_models</b>.\n",
    "<ul>\n",
    "    <li><b>v31</b>: The 31 feature variables per Nguyen</b>\"</li>\n",
    "    <li><b>v31_minmax</b>:The min_max scaled 31 variables into the range of (0,1).</li>\n",
    "    <li><b>IOA</b>:The octave band filtered, A-weighted data</li>\n",
    "</ul>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data source from an Excel file with three worksheets into pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert rows to collection_data collection\n",
    "- This process is done row-by-row using the insert_one() function\n",
    "- Each row obtains its data from the three sources, two from the dataframes and one from reading the npy file.\n",
    "- An additional index is created based on the stock __id__ field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection, S6000_Data, doesn't exist\n"
     ]
    }
   ],
   "source": [
    "### drop the dataset collection if exists, collection_data\n",
    "try:\n",
    "    db.validate_collection(collection_data)  # Try to validate a collection   \n",
    "    db[collection_data].drop()\n",
    "    print(f\"Collection (table), {collection_data}, dropped\")\n",
    "except pymongo.errors.OperationFailure:  # If the collection doesn't exist\n",
    "        print(f\"Collection, {collection_data}, doesn't exist\") \n",
    "\n",
    "## create a new collection, collection_data, in the database, db_name\n",
    "coll_data=db[collection_data]   # collection (table) name \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 35)\n",
      "        Site filename    Rating  Prob_AM_RdmFrst  spectralCentroid  \\\n",
      "H1_0001   H1   s1.wav  1.091934         0.174275         63.081251   \n",
      "H1_0002   H1   s2.wav  1.094161         0.019092         57.650009   \n",
      "H1_0003   H1   s3.wav  4.967208         0.648424         75.049887   \n",
      "\n",
      "         spectralCrest  spectralDecrease  spectralEntropy  spectralFlatness  \\\n",
      "H1_0001      28.079092        -92.466205         0.143241          0.010610   \n",
      "H1_0002      30.680069       -259.422066         0.067829          0.011763   \n",
      "H1_0003      25.424498        -22.843544         0.217764          0.008283   \n",
      "\n",
      "         spectralFlux  ...          PR   Fo  AMfactor       DAM  \\\n",
      "H1_0001  1.087143e-07  ...   69.138538  0.9  0.239249  1.339453   \n",
      "H1_0002  4.547078e-05  ...    7.843309  0.8  0.111174  1.217609   \n",
      "H1_0003  3.236673e-06  ...  104.108914  0.7  0.827716  7.589734   \n",
      "\n",
      "         peakloc_unweightedSPL       L63      L125      L250      L500  \\\n",
      "H1_0001                    0.2  3.403435  2.685078  3.967624  1.748563   \n",
      "H1_0002                    0.5  6.255368  6.074805  3.424099  2.094934   \n",
      "H1_0003                    2.8  7.478385  6.192284  6.185724  5.644156   \n",
      "\n",
      "             L1000  \n",
      "H1_0001   1.427153  \n",
      "H1_0002   1.888449  \n",
      "H1_0003  13.776354  \n",
      "\n",
      "[3 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "### For the 31 feature variables in Nguyen 2021 ########################################\n",
    "## Retrieve the source data from the Excel file, infile_name1\n",
    "df_v31=pd.read_excel(infile_name1,header=0,index_col=0)\n",
    "print(df_v31.shape)\n",
    "print(df_v31.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 31)\n",
      "         spectralCentroid  spectralCrest  spectralDecrease  spectralEntropy  \\\n",
      "H1_0001          0.012519       0.871260          0.999324         0.142351   \n",
      "H1_0002          0.008121       0.957792          0.998104         0.065464   \n",
      "H1_0003          0.022211       0.782944          0.999833         0.218333   \n",
      "\n",
      "         spectralFlatness  spectralFlux  spectralKurtosis  \\\n",
      "H1_0001          0.010966      0.001331          0.004294   \n",
      "H1_0002          0.012185      0.557133          0.002559   \n",
      "H1_0003          0.008507      0.039656          0.000540   \n",
      "\n",
      "         spectralRolloffPoint  spectralSkewness  spectralSlope  ...        PR  \\\n",
      "H1_0001              0.021432          0.078659       0.997918  ...  0.064817   \n",
      "H1_0002              0.007925          0.068858       0.622292  ...  0.007353   \n",
      "H1_0003              0.040809          0.027030       0.967046  ...  0.097601   \n",
      "\n",
      "               Fo  AMfactor       DAM  peakloc_unweightedSPL       L63  \\\n",
      "H1_0001  1.000000  0.047060  0.015886               0.000000  0.046839   \n",
      "H1_0002  0.888889  0.017838  0.013268               0.065217  0.099330   \n",
      "H1_0003  0.777778  0.181323  0.150206               0.565217  0.121841   \n",
      "\n",
      "             L125      L250      L500     L1000  \n",
      "H1_0001  0.034494  0.050674  0.015160  0.008636  \n",
      "H1_0002  0.100103  0.039823  0.022587  0.018756  \n",
      "H1_0003  0.102376  0.094960  0.098695  0.279557  \n",
      "\n",
      "[3 rows x 31 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.//trained_models//scalers//Minmax_scaler_v31_S6000.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Minmax scaling the 31 feasure variables: columns of df_v31[4:35]\n",
    "## The first 4 columns are not scaled. \n",
    "## Check if the first three rows of the scaled data are the same as those from df_v31minmax_veri\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_v31minmax = scaler.fit_transform(df_v31.iloc[:,4:35])\n",
    "df_v31minmax = pd.DataFrame(df_v31minmax, columns=df_v31.columns[4:35], index=df_v31.index)\n",
    "print(df_v31minmax.shape)\n",
    "## Check the first three rows of the scaled data with those from df_v31minmax_veri\n",
    "print(df_v31minmax.head(3))\n",
    "## Save the scaler to be used for scaling new data\n",
    "import joblib\n",
    "joblib.dump(scaler, './/trained_models//scalers//Minmax_scaler_v31_S6000.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.28259413e+03  3.19487568e+01 -9.49792591e-03  9.84429088e-01\n",
      "  9.46154743e-01  8.16156375e-05  3.30122735e+05  3.24719115e+03\n",
      "  3.13545090e+02  7.11390135e-14  1.17464580e+03  4.00000000e+02\n",
      "  8.58377337e-01  7.15270475e+01  2.84255734e+00  2.46119394e+00\n",
      "  3.06684184e+01  1.05000000e+00  5.63476848e+02  1.74230837e+01\n",
      " -1.19318803e-01  1.06667625e+03  9.00000000e-01  4.41591099e+00\n",
      "  4.71328706e+01  4.80000000e+00  5.51901854e+01  5.25686596e+01\n",
      "  5.15162113e+01  4.76758433e+01  4.66158008e+01]\n",
      "[ 1.28259413e+03  3.19487568e+01 -9.49792591e-03  9.84429088e-01\n",
      "  9.46154743e-01  8.16156375e-05  3.30122735e+05  3.24719115e+03\n",
      "  3.13545090e+02  7.11390135e-14  1.17464580e+03  4.00000000e+02\n",
      "  8.58377337e-01  7.15270475e+01  2.84255734e+00  2.46119394e+00\n",
      "  3.06684184e+01  1.05000000e+00  5.63476848e+02  1.74230837e+01\n",
      " -1.19318803e-01  1.06667625e+03  9.00000000e-01  4.41591099e+00\n",
      "  4.71328706e+01  4.80000000e+00  5.51901854e+01  5.25686596e+01\n",
      "  5.15162113e+01  4.76758433e+01  4.66158008e+01]\n",
      "[ 4.76201728e+01  1.89076018e+00 -1.36844624e+05  3.62194891e-03\n",
      "  2.36511625e-04  9.46508682e-11  2.52407855e+00  4.73900685e+01\n",
      "  7.41037561e-01 -6.63293556e-09  7.85295127e+00  5.01231624e+01\n",
      "  0.00000000e+00  1.25115790e+01  7.30995829e-01  9.72035106e-01\n",
      " -1.35027882e+00  5.00000000e-02  1.59143475e-02  8.90980237e-02\n",
      " -1.46909583e+01  0.00000000e+00  0.00000000e+00  3.29898084e-02\n",
      "  6.00228305e-01  2.00000000e-01  8.58582623e-01  9.02931561e-01\n",
      "  1.42951132e+00  1.04159511e+00  1.03352088e+00]\n",
      "[ 4.76201728e+01  1.89076018e+00 -1.36844624e+05  3.62194891e-03\n",
      "  2.36511625e-04  9.46508682e-11  2.52407855e+00  4.73900685e+01\n",
      "  7.41037561e-01 -6.63293556e-09  7.85295127e+00  5.01231624e+01\n",
      "  0.00000000e+00  1.25115790e+01  7.30995829e-01  9.72035106e-01\n",
      " -1.35027882e+00  5.00000000e-02  1.59143475e-02  8.90980237e-02\n",
      " -1.46909583e+01  0.00000000e+00  0.00000000e+00  3.29898084e-02\n",
      "  6.00228305e-01  2.00000000e-01  8.58582623e-01  9.02931561e-01\n",
      "  1.42951132e+00  1.04159511e+00  1.03352088e+00]\n"
     ]
    }
   ],
   "source": [
    "## Check if two scalers created at different times and saved in two locations are the same\n",
    "\n",
    "# Load the joblib file\n",
    "file1 = joblib.load('.//trained_models//scalers//Minmax_scaler_v31_S6000.joblib')\n",
    "file2 = joblib.load('.//trained_models//Minmax_scaler_v31_S6000.joblib')\n",
    "# Print or inspect the content of the file\n",
    "print(file1.data_max_)\n",
    "print(file2.data_max_)\n",
    "\n",
    "print(file1.data_min_)\n",
    "print(file2.data_min_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store data in the collection Data_S6000\n",
    "- First three columns: _id, site, file_name\n",
    "- v31: 31 columns for the original 31 feature variables\n",
    "- IOA: four bands 25-100Hz,50-200Hz, 100-400Hz, 200-800Hz, each with 100 values (No threshold, 819 Hz frame version), 100 values for each sample in each band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For the 31 feature variables and the 31 scaled feature variables ########################################\n",
    "v31_keys=list(df_v31minmax.columns)   # variable names (or column names in the collection), v31 and v31_minmax share the same keys\n",
    "### For reading and organizing IOA data ###################################################################\n",
    "dir_npy='.//Windfarm_IOA//IOA_NPY_data_S6000_819//'  # A-weighted 819 frame rate data\n",
    "bands = ['025_100Hz','050_200Hz', '100_400Hz', '200_800Hz']\n",
    "post_fix=['_025_100Hz.npy','_050_200Hz.npy', '_100_400Hz.npy', '_200_800Hz.npy']\n",
    "IOA_keys = ['v'+str(i).zfill(3) for i in range(100)]   # for the 100 values extracted from 819 rate time series data\n",
    "#############################################################################################################\n",
    "#df_v31.head(3)\n",
    "for i, idx in enumerate(df_v31.index):\n",
    "    pymongo_data_dict=dict()\n",
    "    row = df_v31.loc[idx]    # v31\n",
    "    row_minmax = df_v31minmax.loc[idx]   #v31_minmax\n",
    "    \n",
    "    #print(f\"{i}: {idx}\")\n",
    "\n",
    "    ### file information\n",
    "    pymongo_data_dict[\"_id\"]=idx\n",
    "    pymongo_data_dict[\"site\"]=row[\"Site\"]\n",
    "    pymongo_data_dict[\"file_name\"]=idx+\".wav\"\n",
    "\n",
    "    pymongo_data_dict[\"v31\"]=dict(); \n",
    "    pymongo_data_dict[\"IOA_3rdOctave\"]=dict()\n",
    "\n",
    "    for key in v31_keys:  #  collect the values of the V31 and V31_minmax variables\n",
    "        pymongo_data_dict[\"v31\"][key]=row[key]          \n",
    "        \n",
    "    ## for IOA data, collect the values of the 4 bands for each file\n",
    "    for (band, pos) in zip(bands, post_fix):\n",
    "        pymongo_data_dict['IOA_3rdOctave'][band]=list(np.load(dir_npy+idx+pos))\n",
    "\n",
    "\n",
    "    ## insert the document into the collection\n",
    "    coll_data.insert_one(pymongo_data_dict)\n",
    "    #if i>3: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Part 2: Add analysis results from S6000 to collection_data</b> which includes\n",
    "<ol>\n",
    "<li>scores: The rating and Random forest result from Nguyen 2021 based on 31 summary variables.</li>\n",
    "<li>cluster_v31_minmax: clustering results based the minmax scaled 31 variables, K=2,..., 10</li>\n",
    "<li>IOA rating: two ratings (IOA rating, AMWG) for each band, (1) prominence, (2) modulation depth (L5-L95)\n",
    "<li>YAMnet classes, probs: the top five classes and corresponding probabilities from YAMNET\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Result database content\n",
    "In the update statement, the last 2 True/False fields specifies the upsert and multi flags.\n",
    "\n",
    "- Upsert flag: If set to true, creates a new document when no document matches the query criteria.\n",
    "\n",
    "- Multi flag: If set to true, updates multiple documents that meet the query criteria. If set to false, updates one document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['H1_0001', 'H1_0002', 'H1_0003', 'H1_0004', 'H1_0005'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v31.index[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result 1: From the nguyen's paper\n",
    "## These two scores are also in the collection, collection_data (Data_S6000)\n",
    "df_v31.head(3)  \n",
    "scores = [\"Rating\",\"Prob_AM_RdmFrst\"]\n",
    "for i, idx in enumerate(df_v31.index):\n",
    "    pymongo_result_dict=dict()\n",
    "    #print(f\"{i}: {idx}\")\n",
    "    row = df_v31.loc[idx]\n",
    "\n",
    "\n",
    "    pymongo_result_dict[\"nguyen_scores\"]=dict()\n",
    "    ###############################\n",
    "    for s in scores:\n",
    "        pymongo_result_dict[\"nguyen_scores\"][s]=row[s]\n",
    "    ## insert the document into the collection\n",
    "\n",
    "    coll_data.update_one({\"_id\":idx },{'$set' : pymongo_result_dict}, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         K=2  K=3  K=4  K=5  K=6  K=7  K=8  K=9  K=10\n",
      "H1_0001    0    2    3    0    5    6    0    0     3\n",
      "H1_0002    1    0    1    4    2    5    3    3     7\n",
      "H1_0003    0    1    3    0    5    6    0    1     2\n",
      "(6000, 9)\n",
      "         K=2  K=3  K=4  K=5  K=6  K=7  K=8  K=9  K=10\n",
      "H1_0001    0    2    3    0    5    6    0    0     3\n",
      "H1_0002    1    0    1    4    2    5    3    3     7\n",
      "H1_0003    0    1    3    0    5    6    0    1     2\n",
      "(6000, 9)\n"
     ]
    }
   ],
   "source": [
    "## Result 2: Clustering result by Kmeans with K=2, 3, .., 10 and save the clustering models\n",
    "### Cluster the 6000 samples into 2-10 clusters\n",
    "df_S6000_cluster, _ =cluster_analysis(df_v31minmax, [2,3,4,5,6,7,8,9,10], random_state=42, scaler_type=None,n_init=10, max_iter=300, tol=0.0001,\n",
    "                                 save_model_dir='.//trained_models//clustering//', model_file_name='Kmeans_S6000_v31_MinMax_K=')\n",
    "print(df_S6000_cluster.head(3))\n",
    "print(df_S6000_cluster.shape)\n",
    "\n",
    "## Double check with the saved clustering result\n",
    "infile_name3 = \".//Results//cluster_allotment//Cluster_S6000_MinMax_result.xlsx\"\n",
    "df_cluster=pd.read_excel(infile_name3,header=0,index_col=0)\n",
    "print(df_cluster.head(3))\n",
    "print(df_cluster.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01485179 0.88585969 0.9987002  0.12869654 0.01196497 0.07153502\n",
      "  0.00350418 0.02499423 0.06059704 0.93350133 0.06555377 0.98953944\n",
      "  0.02435714 0.44514964 0.39084459 0.2639636  0.47558703 0.58648474\n",
      "  0.00952726 0.06958317 0.91977739 0.01539305 0.77185695 0.06721571\n",
      "  0.04312486 0.13660059 0.11806475 0.09896513 0.06189822 0.06190411\n",
      "  0.06980546]\n",
      " [0.00587739 0.92736801 0.99828461 0.08563632 0.00369299 0.01793724\n",
      "  0.00892482 0.01007855 0.10272272 0.98385129 0.03304128 0.99906487\n",
      "  0.06550112 0.27118861 0.56725939 0.48925307 0.65266533 0.49569536\n",
      "  0.00559861 0.05198528 0.9412403  0.00942295 0.70364238 0.05355128\n",
      "  0.04020534 0.12825727 0.12898097 0.08962586 0.05620599 0.05567665\n",
      "  0.06124672]]\n"
     ]
    }
   ],
   "source": [
    "model=joblib.load('.//trained_models//clustering//Kmeans_S6000_v31_MinMax_K=5.joblib')\n",
    "print(model.cluster_centers_[:2])  # get the cluster centers of the first two clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Result 2: Clustering result\n",
    "infile_name3 = \".//Results//cluster_allotment//Cluster_S6000_MinMax_result.xlsx\"\n",
    "df_cluster=pd.read_excel(infile_name3,header=0,index_col=0)\n",
    "df_cluster.head(3)\n",
    "\n",
    "pymongo_cluster_dict=dict()\n",
    "for i, idx in enumerate(df_S6000_cluster.index):\n",
    "    pymongo_cluster_dict[idx]=dict()\n",
    "    #print(f\"{i}: {idx}\")\n",
    "    row = df_cluster.loc[idx]\n",
    "\n",
    "    ## mongodb does not allow np.int64 as a value, chagne it to int\n",
    "    pymongo_cluster_dict[idx][\"K=2\"]=int(row[\"K=2\"])\n",
    "    pymongo_cluster_dict[idx][\"K=3\"]=int(row[\"K=3\"])\n",
    "    pymongo_cluster_dict[idx][\"K=4\"]=int(row[\"K=4\"])\n",
    "    pymongo_cluster_dict[idx][\"K=5\"]=int(row[\"K=5\"])\n",
    "    pymongo_cluster_dict[idx][\"K=6\"]=int(row[\"K=6\"])\n",
    "    pymongo_cluster_dict[idx][\"K=7\"]=int(row[\"K=7\"])\n",
    "    pymongo_cluster_dict[idx][\"K=8\"]=int(row[\"K=8\"])\n",
    "    pymongo_cluster_dict[idx][\"K=9\"]=int(row[\"K=9\"])\n",
    "    pymongo_cluster_dict[idx][\"K=10\"]=int(row[\"K=10\"])\n",
    "    \n",
    "\n",
    "    ## Append the new result for idx to the document \n",
    "    #print(i, idx, pymongo_cluster_dict[idx])\n",
    "    coll_data.update_one({\"_id\":idx },{'$set' : {\"cluster_v31_minmax\": pymongo_cluster_dict[idx]}}, False, False)\n",
    "    #if i>3: break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', '025_100Hz', '050_200Hz', '100_400Hz', '200_800Hz']\n"
     ]
    }
   ],
   "source": [
    "## Result 3: from IOA results\n",
    "infile_name4=\".//Windfarm_IOA//IOA_Result_819_S6000//All6000_IOA_AW_819_NoThreshold.xlsx\"\n",
    "xls = pd.ExcelFile(infile_name4)\n",
    "sheet_names = xls.sheet_names  # see all sheet names\n",
    "print(sheet_names)\n",
    "keys=None\n",
    "\n",
    "### Get the data from each sheet\n",
    "df_bands = dict()\n",
    "for sht in sheet_names[1:]:\n",
    "    df_bands[sht]=pd.read_excel(infile_name4,sheet_name=sht,header=0,index_col=0)\n",
    "    \n",
    "    #print(df_bands[sht].head(3))\n",
    "    if sht==sheet_names[1]:\n",
    "        indices = df_bands[sht].index  # get the index names of the first sheet, since they are the same for all sheets\n",
    "    \n",
    "keys = ['prominence','L5-L95']   # get the column names of the two IOA values to extract\n",
    "\n",
    "############################################################\n",
    "\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    pymongo_sht_dict=dict()\n",
    "    for sht in sheet_names[1:]:\n",
    "        pymongo_sht_dict[sht]=dict()\n",
    "        ## find the index of the row in coll_result, with the same id\n",
    "        row = df_bands[sht].loc[idx]\n",
    "        #print(f\"sht = {sht}, idx = {idx}\")\n",
    "\n",
    "        for key in keys:  # skip the first first column, 'band', which is the parent key\n",
    "            pymongo_sht_dict[sht][key]=row[key]\n",
    "        ## append the document to the end of the document with the same id if the document with the same id exists, otherwise show error message\n",
    "\n",
    "    #print(pymongo_sht_dict)\n",
    "    coll_data.update_one({\"_id\":idx },{'$set' : {\"IOA_rating\":pymongo_sht_dict}}, False, False)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['top_1', 'top_2', 'top_3', 'top_4', 'top_5'], dtype='object') Index(['prob_1', 'prob_2', 'prob_3', 'prob_4', 'prob_5'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsaic\\AppData\\Local\\Temp\\ipykernel_28692\\3159227334.py:32: DeprecationWarning: update is deprecated. Use replace_one, update_one or update_many instead.\n",
      "  coll_data.update({\"_id\":idx },{'$set' : {\"YAMNET_class\": pymongo_YAMNET_dict}}, True, True)\n"
     ]
    }
   ],
   "source": [
    "## Result 4: YAMNET results\n",
    "infile_name5=\".//Results//classification//YAMNET_cls_results.xlsx\"\n",
    "df_H1=pd.read_excel(infile_name5,sheet_name=\"H1_cls\",header=0,index_col=0)\n",
    "df_H2=pd.read_excel(infile_name5,sheet_name=\"H2_cls\",header=0,index_col=0)\n",
    "### concatenate the two dataframes\n",
    "df_H=pd.concat([df_H1,df_H2],axis=0)\n",
    "classes=df_H.columns[:5]  # the first 5 columns are the class names\n",
    "probs=df_H.columns[5:]  # the rest of the columns are the probabilities\n",
    "print(classes, probs)\n",
    "#############################################################################\n",
    "#pymongo_class_dict=dict()\n",
    "#pymongo_prob_dict=dict()\n",
    "\n",
    "for i, idx in enumerate(df_H.index):\n",
    "    pymongo_YAMNET_dict=dict()\n",
    "    pymongo_YAMNET_dict['class']=dict()\n",
    "    pymongo_YAMNET_dict['prob']=dict()\n",
    "    ## find the index of the row in coll_result, with the same id\n",
    "    #pymongo_YAMNET_dict['class'][idx]=dict()\n",
    "    #pymongo_YAMNET_dict['prob'][idx]=dict()\n",
    "    row = df_H.loc[idx]\n",
    "\n",
    "    for s in classes:\n",
    "        pymongo_YAMNET_dict['class'][s]=row[s]\n",
    "    for s in probs:\n",
    "        pymongo_YAMNET_dict['prob'][s]=row[s]\n",
    "\n",
    "    #print(pymongo_class_dict[idx])\n",
    "    #print(pymongo_prob_dict[idx])\n",
    "    ## append the document to the end of the document with the same id if the document with the same id exists, \n",
    "    #coll_result.update({\"id\":idx },{'$set' : {\"classes\": pymongo_class_dict[idx], \"probs\":pymongo_prob_dict[idx]}}, True, True)\n",
    "    coll_data.update({\"_id\":idx },{'$set' : {\"YAMNET_class\": pymongo_YAMNET_dict}}, True, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<p style=\"font-size:20px\"><b>Section 3: What have you achieved:</b></p> \n",
    "At this point, we have created a mongoDB collection <b>S6000_Data</b> which constains the following.\n",
    "\n",
    "Data:\n",
    "<ul>\n",
    "    <li><b>_id</b>: using file id as the id for each document, i.e., H1_0001, ... H1_3000 and H2_0001,... H2_3000.</li>\n",
    "    <li><b>site</b>: Either H1 or H2, representing Farm 1 or Farm 2</li>\n",
    "    <li><b>file name</b>: The corresponding wav file name with extension wav, for example, H1_0001.wav which is the id with \".wav\".</li>\n",
    "    <li><b>v31</b>: The 31 feature variables from Nguyen et. al., 2021.</li>\n",
    "    <li><b>IOA_3rdOctave</b>: The filtered 3rd octave band data with a 100 milisecond frame.</li>\n",
    "</ul>\n",
    "\n",
    "Analysis Result:\n",
    "<ul>\n",
    "    <li><b>nguyen scores</b>: The 1 to 5 rating scores in Nguyen and the probability of AM existence from Random Forest analysis.</li>\n",
    "    <li><b>cluster_v31_minmax</b>: Cluster id for K=2, 3, ..., 10 using the minmax scaled 31 variables.</li>\n",
    "    <li><b>IOA_rating</b>: The Prominence and Modulation Depth ratings on the IOA_3rdOctave data.</li>\n",
    "    <li><b>YAMNET_class</b>: The top five classes picked by YAMNET and their corresponding probabilities</li>    \n",
    "</ul>\n",
    "\n",
    "Sklearn models created and saved: path = './/trained_models//model_type//' where model_type is either scaler or clustering\n",
    "<ul>\n",
    "    <li><b>MinMax_scaler</b>: MinMax scaler based on the original 31 feature variables.</li>\n",
    "    <li><b>KMeans cluster on v31_minmax</b>: Nine clustering model for K=2, 3, ..., 10 using the minmax scaled 31 variables.</li>   \n",
    "</ul>\n",
    "</div>   "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce7598402296efdb2a2a3ae171a6e3871c2d8faeca73f35326143a0f1e59ad6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
