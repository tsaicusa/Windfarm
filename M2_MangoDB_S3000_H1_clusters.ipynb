{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on H1: the 3000 samples from Farm 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on converting Jupyter Notebook output to MS Word Documents\n",
    "\n",
    "- The best way to convert the ipynb file (Jupyter Notebook) to a docx file is to follow the two step approach explained in: https://blog.ouseful.info/2017/06/13/using-jupyter-notebooks-for-assessment-export-as-word-docx-extension/, i.e. run the following two commands on the anaconda command line\n",
    "- Step 1: $ jupyter nbconvert --no-input --to html file_name.ipynb (use --no-input to exclude code cells, i.e. convert only markdown cells)\n",
    "\n",
    "- Step 2: $ pandoc -s file_name.html -o file_name.docx\n",
    "- This approach produces, by far, the best quality docx ouput, no distortion of either the text or the graphs.  The only drawback is that it include the hidden code cells made by the \"hide all\" extension.  I have to manually delete those contents from the produced docx file.\n",
    "#### convert ijynb to a docx file\n",
    "- https://nbconvert.readthedocs.io/en/latest/index.html\n",
    "- install nbconvert [pip install nbconvert]\n",
    "- install pandoc: [https://github.com/jgm/pandoc/releases/tag/3.1.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "oneDrive_root={}\n",
    "oneDrive_root[1]=\"C:\\\\Users\\\\Chihyang\\OneDrive for Business\\\\\"\n",
    "oneDrive_root[2]=\"C:\\\\Users\\\\Chihyang\\OneDrive for Business\\\\\"\n",
    "oneDrive_root[3]=\"C:\\\\Users\\\\tsaic\\\\OneDrive - State University of New York at New Paltz\\\\\"  # laptop\n",
    "\n",
    "site=3   # the short or long business OneDrive directory name\n",
    "lib_dir=oneDrive_root[site]+'\\\\Prudentia\\\\Tsaipy'\n",
    "# append additional library path for this study\n",
    "sys.path.append(lib_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import sklearn as sk\n",
    "#from sklearn.cluster import KMeans\n",
    "from dbconnect.mongodb import cursor_to_dataframe3\n",
    "from WFN_lib.WindFarm import distance2center,  write_df_sub2Excel\n",
    "from WFN_lib.mongodb_util import flatten_dictionary\n",
    "from WFN_lib.cluster_classify import cluster_analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "from dbconnect import mongodb as mdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>MongoDb Parameters:</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in coll_data = 6000\n",
      "Rows in coll_res = 6000\n"
     ]
    }
   ],
   "source": [
    "## pymongo connect to mongodb database and collection\n",
    "db_name = \"Windfarm_6000\"\n",
    "client = MongoClient('localhost', 27017)  # connect to the db engine\n",
    "db = client[db_name]\n",
    "coll_data=db['Data_S6000']\n",
    "coll_res = db['Results_S6000']\n",
    "print(f\"Rows in coll_data = {coll_data.count_documents({})}\")\n",
    "print(f\"Rows in coll_res = {coll_res.count_documents({})}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>I. Clustering the 3000 cases in H1</b>\n",
    "\n",
    "<b>DO NOT</b> use <b>v31_minmax</b> obtained from S6000 since it is scaled with the whole 6000 cases.  <b>For validation purpose</b> with H2 files, we need to scale v31 with only the 3000 cases in H1.\n",
    "\n",
    "Thus, for the S3000 database, we only store v31_minmax variables scaled based on the 3000 files in H1.  If you need variable values in the original scale, just retrieve them from collection <b>Data_S6000</b> with the indices H1_xxxx, where xxxx ranges from <b>H1_0001</b> to <b>H1_3000</b>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data from H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 31)\n",
      "         v31.spectralCentroid  v31.spectralCrest  v31.spectralDecrease  \\\n",
      "H1_0001             63.081251          28.079092            -92.466205   \n",
      "H1_0002             57.650009          30.680069           -259.422066   \n",
      "H1_0003             75.049887          25.424498            -22.843544   \n",
      "\n",
      "         v31.spectralEntropy  v31.spectralFlatness  v31.spectralFlux  \\\n",
      "H1_0001             0.143241              0.010610      1.087143e-07   \n",
      "H1_0002             0.067829              0.011763      4.547078e-05   \n",
      "H1_0003             0.217764              0.008283      3.236673e-06   \n",
      "\n",
      "         v31.spectralKurtosis  v31.spectralRolloffPoint  v31.spectralSkewness  \\\n",
      "H1_0001           1420.161272                115.968033             25.345770   \n",
      "H1_0002            847.458521                 72.747519             22.280157   \n",
      "H1_0003            180.938195                177.970026              9.196154   \n",
      "\n",
      "         v31.spectralSlope  ...      v31.PR  v31.Fo  v31.AMfactor   v31.DAM  \\\n",
      "H1_0001      -1.373689e-11  ...   69.138538     0.9      0.239249  1.339453   \n",
      "H1_0002      -2.505266e-09  ...    7.843309     0.8      0.111174  1.217609   \n",
      "H1_0003      -2.185107e-10  ...  104.108914     0.7      0.827716  7.589734   \n",
      "\n",
      "         v31.peakloc_unweightedSPL   v31.L63  v31.L125  v31.L250  v31.L500  \\\n",
      "H1_0001                        0.2  3.403435  2.685078  3.967624  1.748563   \n",
      "H1_0002                        0.5  6.255368  6.074805  3.424099  2.094934   \n",
      "H1_0003                        2.8  7.478385  6.192284  6.185724  5.644156   \n",
      "\n",
      "         v31.L1000  \n",
      "H1_0001   1.427153  \n",
      "H1_0002   1.888449  \n",
      "H1_0003  13.776354  \n",
      "\n",
      "[3 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "res = coll_data.find({'site':'H1'},{'v31':1})  # index '_id' is automatically added\n",
    "df_H1 = cursor_to_dataframe3(res,\"_id\")\n",
    "print(df_H1.shape)\n",
    "print(df_H1.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use the unscaled data and ask the function to scale it based on the 3000 files\n",
    "## df_res is the dataframe with the cluster allotment; df_scaled is the scaled data\n",
    "## Save the trained KMeans model to the directory save_model_dir\n",
    "df_res, df_H1_scaled=cluster_analysis(df_H1, [3,4,5,6], random_state=42, scaler_type=\"MinMax\",n_init=10, max_iter=300, tol=0.0001,\n",
    "                                   save_model_dir='.//trained_models//', model_file_name='Kmeans_H1_S3000_v31_MinMax_K=', \n",
    "                                   scaler_model_name='_scaler_v31_H1_S3000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         K=3  K=4  K=5  K=6\n",
      "H1_0001    1    2    0    3\n",
      "H1_0002    2    1    1    1\n",
      "H1_0003    0    2    0    3\n",
      "         v31.spectralCentroid  v31.spectralCrest  v31.spectralDecrease  \\\n",
      "H1_0001              0.017638           0.815540              0.996951   \n",
      "H1_0002              0.011442           0.939524              0.991424   \n",
      "H1_0003              0.031291           0.689000              0.999256   \n",
      "\n",
      "         v31.spectralEntropy  v31.spectralFlatness  v31.spectralFlux  \\\n",
      "H1_0001             0.228597              0.029063          0.001635   \n",
      "H1_0002             0.105126              0.032293          0.684277   \n",
      "H1_0003             0.350613              0.022545          0.048706   \n",
      "\n",
      "         v31.spectralKurtosis  v31.spectralRolloffPoint  v31.spectralSkewness  \\\n",
      "H1_0001              0.006851                  0.023762              0.077191   \n",
      "H1_0002              0.004065                  0.008786              0.067042   \n",
      "H1_0003              0.000823                  0.045246              0.023726   \n",
      "\n",
      "         v31.spectralSlope  ...    v31.PR    v31.Fo  v31.AMfactor   v31.DAM  \\\n",
      "H1_0001           0.997410  ...  0.064817  1.000000      0.047060  0.015886   \n",
      "H1_0002           0.526505  ...  0.007353  0.888889      0.017838  0.013268   \n",
      "H1_0003           0.958707  ...  0.097601  0.777778      0.181323  0.150206   \n",
      "\n",
      "         v31.peakloc_unweightedSPL   v31.L63  v31.L125  v31.L250  v31.L500  \\\n",
      "H1_0001                   0.000000  0.046839  0.034494  0.040067  0.005090   \n",
      "H1_0002                   0.065217  0.099330  0.100103  0.029094  0.012593   \n",
      "H1_0003                   0.565217  0.121841  0.102376  0.084847  0.089479   \n",
      "\n",
      "         v31.L1000  \n",
      "H1_0001   0.007652  \n",
      "H1_0002   0.017782  \n",
      "H1_0003   0.278842  \n",
      "\n",
      "[3 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_res.head(3))\n",
    "print(df_H1_scaled.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_H1_clusters, cluster_size_H1, cluster_means_H1, cluster_vars_H1 = distance2center(df_H1_scaled, df_res, [3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K=3</th>\n",
       "      <th>K=4</th>\n",
       "      <th>K=5</th>\n",
       "      <th>K=6</th>\n",
       "      <th>dist_K=3</th>\n",
       "      <th>rank_K=3</th>\n",
       "      <th>dist_K=4</th>\n",
       "      <th>rank_K=4</th>\n",
       "      <th>dist_K=5</th>\n",
       "      <th>rank_K=5</th>\n",
       "      <th>dist_K=6</th>\n",
       "      <th>rank_K=6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>H1_0001</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.539937</td>\n",
       "      <td>1383</td>\n",
       "      <td>0.552750</td>\n",
       "      <td>934</td>\n",
       "      <td>0.565479</td>\n",
       "      <td>980</td>\n",
       "      <td>0.557639</td>\n",
       "      <td>973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H1_0002</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.557416</td>\n",
       "      <td>807</td>\n",
       "      <td>0.585724</td>\n",
       "      <td>740</td>\n",
       "      <td>0.589200</td>\n",
       "      <td>730</td>\n",
       "      <td>0.409207</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H1_0003</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.603725</td>\n",
       "      <td>109</td>\n",
       "      <td>0.601751</td>\n",
       "      <td>966</td>\n",
       "      <td>0.613486</td>\n",
       "      <td>1006</td>\n",
       "      <td>0.609877</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         K=3  K=4  K=5  K=6  dist_K=3  rank_K=3  dist_K=4  rank_K=4  dist_K=5  \\\n",
       "H1_0001    1    2    0    3  0.539937      1383  0.552750       934  0.565479   \n",
       "H1_0002    2    1    1    1  0.557416       807  0.585724       740  0.589200   \n",
       "H1_0003    0    2    0    3  0.603725       109  0.601751       966  0.613486   \n",
       "\n",
       "         rank_K=5  dist_K=6  rank_K=6  \n",
       "H1_0001       980  0.557639       973  \n",
       "H1_0002       730  0.409207       328  \n",
       "H1_0003      1006  0.609877       997  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## df_H1: for each sample, this dataframe contains the cluster number, the distance to the cluster center, \n",
    "##   and the ranking of distance to cluster center (closest to farthest).  \n",
    "## !! Note that measure of the distance and ranking are made with respect to the cluster the sample is in.\n",
    "print(df_H1_clusters.shape)\n",
    "df_H1_clusters.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Write the K=5 Clustering result with additional details into an Excel file:</b> Data in a cluster is presented in one Worksheet of the file.   \n",
    "\n",
    "In each cluster, rows are sorted from the nearest to its class center the farthest.  For each row (case), columns include \n",
    "<ol>\n",
    "<li> Cluster it is assigned to,</li>\n",
    "<li>distance to cluster center,</li>\n",
    "<li>ranking of distance to center,</li>\n",
    "<li>two IOA ratings (prominenace and modulation depth) for each band (four bands), and</li> \n",
    "<li>YAMnet top five classes with corresponding probabilities (10 columns)</li>\n",
    "</ol>\n",
    "\n",
    "<b>Cluster Characteristics</b>:\n",
    "<ul>\n",
    "<li>Cluster 0: 1061 cases; Low confidence even to the first YAMnet class,</li>\n",
    "<li>Cluster 1: 837 cases; Low confidence even to the first YAMnet class,</li>\n",
    "<li>Cluster 2: 58 cases; High confidence to the first YAMnet class; first class has lots of \"silence\"</li>\n",
    "<li>Cluster 3: 814 cases; Moderate confidence to the first YAMnet class; first class has lots of \"silence\", and</li> \n",
    "<li>Cluster 4: 230 cases; Moderate confidence to the first YAMnet class; first class has lots of \"Animal\"</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details retrieved from S6000 has shape:  (3000, 18)\n",
      "Joint dataframe has shape:  (3000, 21)\n",
      "         K=5  dist_K=5  rank_K=5  IOA_rating.025_100Hz.prominence  \\\n",
      "H1_0001    0  0.565479       980                        14.307293   \n",
      "H1_0002    1  0.589200       730                         5.458717   \n",
      "\n",
      "         IOA_rating.025_100Hz.L5-L95  IOA_rating.050_200Hz.prominence  \\\n",
      "H1_0001                     3.114064                        25.675691   \n",
      "H1_0002                     3.831529                         4.284061   \n",
      "\n",
      "         IOA_rating.050_200Hz.L5-L95  IOA_rating.100_400Hz.prominence  \\\n",
      "H1_0001                     1.621372                        36.235453   \n",
      "H1_0002                     3.762765                         1.733793   \n",
      "\n",
      "         IOA_rating.100_400Hz.L5-L95  IOA_rating.200_800Hz.prominence  ...  \\\n",
      "H1_0001                     1.477713                        50.222898  ...   \n",
      "H1_0002                     2.792567                         2.223483  ...   \n",
      "\n",
      "         YAMnet.class.top_1 YAMnet.class.top_2  YAMnet.class.top_3  \\\n",
      "H1_0001             Silence              Music  Musical instrument   \n",
      "H1_0002              Animal       Wild animals                Bird   \n",
      "\n",
      "        YAMnet.class.top_4  YAMnet.class.top_5 YAMnet.prob.prob_1  \\\n",
      "H1_0001             Speech  Inside, small room           0.998916   \n",
      "H1_0002        White noise                 Owl           0.209681   \n",
      "\n",
      "         YAMnet.prob.prob_2  YAMnet.prob.prob_3  YAMnet.prob.prob_4  \\\n",
      "H1_0001            0.000025            0.000008            0.000008   \n",
      "H1_0002            0.191000            0.174014            0.160025   \n",
      "\n",
      "         YAMnet.prob.prob_5  \n",
      "H1_0001            0.000008  \n",
      "H1_0002            0.129709  \n",
      "\n",
      "[2 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "## First, filter the columns to keep only those K=5 columns\n",
    "k=5\n",
    "## get the columns whose names contain K=5\n",
    "df_H1_K = df_H1_clusters.filter(regex='K='+str(k))\n",
    "#print(df_H1_K.head(2))\n",
    "\n",
    "## Retrieve IOA rating and YAMnet classification data from S6000\n",
    "res=db['Results_S6000'].find({'_id':{'$in':list(df_H1_clusters.index)}},{'IOA_rating':1, 'YAMnet':1})\n",
    "#print(db['Results_S6000'].index_information())\n",
    "#print(res.count())\n",
    "df_IOA_YAM_S6000 = cursor_to_dataframe3(res,\"_id\")\n",
    "print(\"Details retrieved from S6000 has shape: \",df_IOA_YAM_S6000.shape)\n",
    "\n",
    "## Join the clustering result dataframe (3000 cases) to the IOA-YAMnet detail dataframe (from S6000) on index\n",
    "df_H1K_IOA_YAM = df_H1_K.join(df_IOA_YAM_S6000, how='inner')\n",
    "print(\"Joint dataframe has shape: \",df_H1K_IOA_YAM.shape)\n",
    "print(df_H1K_IOA_YAM.head(2))\n",
    "## write the dataframe to an Excel file,  one worksheet for each cluster \n",
    "#write_df_sub2Excel(df_H1K_IOA_YAM, method='', n_clusters=k, filename=f'.\\\\Results_H1\\\\S3000_Results\\\\H1_S3000_K={k}_IOA_YAM.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 3)\n",
      "         K=5  dist_K=5  rank_K=5\n",
      "H1_0001    0  0.565479       980\n",
      "H1_0002    1  0.589200       730\n",
      "H1_0003    0  0.613486      1006\n",
      "K=5\n",
      "0    1061\n",
      "1     837\n",
      "2      58\n",
      "3     814\n",
      "4     230\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_H1_K.shape)\n",
    "print(df_H1_K.head(3))\n",
    "print(df_H1_K.groupby('K=5').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>### Create a MongoDB collection to store H1 result</b> need to load info from df_H1</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection, Data_Results_H1_S3000, doesn't exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x14ac4293580>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Drop the collection, coll_res_H1 and recreate it anew.\n",
    "coll_res_H1 = 'Data_Results_H1_S3000'\n",
    "try:\n",
    "    db.validate_collection(coll_res_H1)  # Try to validate a collection   \n",
    "    db[coll_res_H1].drop()\n",
    "    print(f\"Collection (table), {coll_res_H1}, dropped\")\n",
    "except pymongo.errors.OperationFailure:  # If the collection doesn't exist\n",
    "        print(f\"Collection, {coll_res_H1}, doesn't exist\") \n",
    "     \n",
    "coll_res_H1 = db[coll_res_H1]\n",
    "### Insert a new row to describe the data in collection_structure\n",
    "coll_struc=db['varStructures']\n",
    "coll_struc.delete_one({\"_id\":\"V31_H1_minmax\"})\n",
    "coll_struc.insert_one({\"_id\":\"V31_H1_minmax\",\"category\":\"scaled_data\",\"file_name\":\"None\",\"directory\":\"None\",\n",
    "                           \"type\":\"31 minmax feature variables and clustering information\",\"in_collection\":\"Data_Results_H1_S3000\",\n",
    "                       \"desciption\":\"Minmax scaled the 31 featured variables using the 3000 files from H1.  Clustering result and distance to center are included.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'H1_0001', 'v31_minmax': {'spectralCentroid': 0.017637594005318696, 'spectralCrest': 0.8155397082188848, 'spectralDecrease': 0.9969509741955754, 'spectralEntropy': 0.22859728551935327, 'spectralFlatness': 0.0290629742847957, 'spectralFlux': 0.0016345910289633525, 'spectralKurtosis': 0.006851094334737899, 'spectralRolloffPoint': 0.023762321379716456, 'spectralSkewness': 0.07719102975275033, 'spectralSlope': 0.9974095650903214, 'spectralSpread': 0.05435743810758327, 'pitch': 1.0, 'harmonicRatio': 0.016149689583707976, 'LA': 0.2167899481574682, 'ratioLGLA': 0.46005860861977876, 'ratioLCLA': 0.38362688786167887, 'diffLCLA': 0.458930768861343, 'peakloc': 0.85, 'peakval': 0.00019967903502990527, 'pos_slope': 0.008018907555842584, 'neg_slope': 0.9887173364902124, 'PR': 0.06481679674502112, 'Fo': 1.0, 'AMfactor': 0.04705981730750444, 'DAM': 0.015886152932799373, 'peakloc_unweightedSPL': 0.0, 'L63': 0.0468392604214644, 'L125': 0.03449379302729967, 'L250': 0.04006677762574156, 'L500': 0.005089660118782563, 'L1000': 0.007652364107307217}, 'cluster_v31_minmax': {'v31.spectralCentroid': 63.08125138349597, 'v31.spectralCrest': 28.07909234965186, 'v31.spectralDecrease': -92.46620548284287, 'v31.spectralEntropy': 0.1432411172923195, 'v31.spectralFlatness': 0.01060983535345411, 'v31.spectralFlux': 1.087143068380079e-07, 'v31.spectralKurtosis': 1420.161271834405, 'v31.spectralRolloffPoint': 115.9680327374134, 'v31.spectralSkewness': 25.34576960205515, 'v31.spectralSlope': -1.373689352365103e-11, 'v31.spectralSpread': 71.27682127929347, 'v31.pitch': 400.0, 'v31.harmonicRatio': 0.01386252753453142, 'v31.LA': 24.70605609893682, 'v31.ratioLGLA': 1.708363534182561, 'v31.ratioLCLA': 1.552163809128295, 'v31.diffLCLA': 13.64179004412631, 'v31.peakloc': 0.9, 'v31.peakval': 0.1372620995733518, 'v31.pos_slope': 0.2280976525732999, 'v31.neg_slope': -0.3015504326880923, 'v31.PR': 69.13853785846995, 'v31.Fo': 0.9, 'v31.AMfactor': 0.2392492786770764, 'v31.DAM': 1.339452976236698, 'v31.peakloc_unweightedSPL': 0.2, 'v31.L63': 3.403434715374615, 'v31.L125': 2.685078491621468, 'v31.L250': 3.967624044590931, 'v31.L500': 1.748562646561531, 'v31.L1000': 1.427152994098332}}\n",
      "{'_id': 'H1_0002', 'v31_minmax': {'spectralCentroid': 0.011441774492367815, 'spectralCrest': 0.9395238433090222, 'spectralDecrease': 0.9914238419435363, 'spectralEntropy': 0.10512609322912811, 'spectralFlatness': 0.03229337606948892, 'spectralFlux': 0.6842773481264546, 'spectralKurtosis': 0.0040652790016583295, 'spectralRolloffPoint': 0.008786377745623703, 'spectralSkewness': 0.06704199830889489, 'spectralSlope': 0.5265051921750052, 'spectralSpread': 0.06331608535611732, 'pitch': 1.0, 'harmonicRatio': 0.0, 'LA': 0.6305857993249097, 'ratioLGLA': 0.3309898176480751, 'ratioLCLA': 0.15652467310955143, 'diffLCLA': 0.3566533644205335, 'peakloc': 0.4, 'peakval': 0.00029952872813645437, 'pos_slope': 0.02538866409296203, 'neg_slope': 0.9860602853207756, 'PR': 0.007353035863895142, 'Fo': 0.888888888888889, 'AMfactor': 0.0178383101687557, 'DAM': 0.013267690943913063, 'peakloc_unweightedSPL': 0.06521739130434784, 'L63': 0.0993304988546141, 'L125': 0.10010258933790098, 'L250': 0.029093835867720068, 'L500': 0.012593008697807398, 'L1000': 0.017782482969207685}, 'cluster_v31_minmax': {'v31.spectralCentroid': 57.65000877097148, 'v31.spectralCrest': 30.68006947130308, 'v31.spectralDecrease': -259.4220659190198, 'v31.spectralEntropy': 0.06782926300045748, 'v31.spectralFlatness': 0.01176284891491848, 'v31.spectralFlux': 4.547077742378843e-05, 'v31.spectralKurtosis': 847.4585213282901, 'v31.spectralRolloffPoint': 72.74751944171882, 'v31.spectralSkewness': 22.28015712348969, 'v31.spectralSlope': -2.5052663771192e-09, 'v31.spectralSpread': 81.72970681467415, 'v31.pitch': 400.0, 'v31.harmonicRatio': 0.0, 'v31.LA': 47.98215381329066, 'v31.ratioLGLA': 1.438125823029731, 'v31.ratioLCLA': 1.217232332641537, 'v31.diffLCLA': 10.42327519802614, 'v31.peakloc': 0.45, 'v31.peakval': 0.1935226184270005, 'v31.pos_slope': 0.5291847639177348, 'v31.neg_slope': -0.3402201226632365, 'v31.PR': 7.843308740641405, 'v31.Fo': 0.8, 'v31.AMfactor': 0.1111737159539453, 'v31.DAM': 1.217609021220894, 'v31.peakloc_unweightedSPL': 0.5, 'v31.L63': 6.255367831732705, 'v31.L125': 6.074804721515292, 'v31.L250': 3.424098863116136, 'v31.L500': 2.094933955160954, 'v31.L1000': 1.888449374997052}}\n",
      "{'_id': 'H1_0003', 'v31_minmax': {'spectralCentroid': 0.03129110086661609, 'spectralCrest': 0.6889997520403727, 'spectralDecrease': 0.9992558567923124, 'spectralEntropy': 0.3506132531823862, 'spectralFlatness': 0.022544837672352054, 'spectralFlux': 0.0487064837294661, 'spectralKurtosis': 0.0008231039933187534, 'spectralRolloffPoint': 0.045246063337586274, 'spectralSkewness': 0.023726035688486375, 'spectralSlope': 0.9587068713127748, 'spectralSpread': 0.06675644905933407, 'pitch': 0.9955658379313356, 'harmonicRatio': 0.012506693849401321, 'LA': 0.5485798501048168, 'ratioLGLA': 0.3602621824857425, 'ratioLCLA': 0.21247349182542852, 'diffLCLA': 0.4385288326272104, 'peakloc': 0.4, 'peakval': 0.020942113612586324, 'pos_slope': 0.14997931246572838, 'neg_slope': 0.7972661519707146, 'PR': 0.09760122937179763, 'Fo': 0.7777777777777778, 'AMfactor': 0.181323346874311, 'DAM': 0.1502064984299628, 'peakloc_unweightedSPL': 0.5652173913043479, 'L63': 0.12184074075262269, 'L125': 0.10237643144043579, 'L250': 0.08484681441557032, 'L500': 0.0894788602494377, 'L1000': 0.2788421741296269}, 'cluster_v31_minmax': {'v31.spectralCentroid': 75.04988735174351, 'v31.spectralCrest': 25.42449843119785, 'v31.spectralDecrease': -22.84354411073368, 'v31.spectralEntropy': 0.2177641716502682, 'v31.spectralFlatness': 0.008283344591457867, 'v31.spectralFlux': 3.236672738263327e-06, 'v31.spectralKurtosis': 180.9381949986676, 'v31.spectralRolloffPoint': 177.9700259084858, 'v31.spectralSkewness': 9.19615435290071, 'v31.spectralSlope': -2.185107461093619e-10, 'v31.spectralSpread': 85.74389857735213, 'v31.pitch': 398.4485893980234, 'v31.harmonicRatio': 0.01073546255825146, 'v31.LA': 43.3693031000223, 'v31.ratioLGLA': 1.499414821045495, 'v31.ratioLCLA': 1.29974592891206, 'v31.diffLCLA': 12.99977204398488, 'v31.peakloc': 0.45, 'v31.peakval': 11.82463033590743, 'v31.pos_slope': 2.688837280573414, 'v31.neg_slope': -3.087856558780686, 'v31.PR': 104.1089135969001, 'v31.Fo': 0.7, 'v31.AMfactor': 0.8277157469127329, 'v31.DAM': 7.589733562293428, 'v31.peakloc_unweightedSPL': 2.8, 'v31.L63': 7.478385353179092, 'v31.L125': 6.192284429283589, 'v31.L250': 6.185723635327136, 'v31.L500': 5.644156376889391, 'v31.L1000': 13.7763543811034}}\n"
     ]
    }
   ],
   "source": [
    "K_columns = df_H1.columns\n",
    "V_columns = df_H1_scaled.columns\n",
    "## Remove the prefix V31. from the column names\n",
    "nV_columns = [col[col.find(\".\")+1:] for col in V_columns]   # new column names after removing the prefix V31.\n",
    "\n",
    "for i in range(len(df_H1)):\n",
    "    row = dict()\n",
    "    row['_id']=df_H1.index[i]\n",
    "    row['v31_minmax']=dict()\n",
    "    row['cluster_v31_minmax']=dict()\n",
    "    ### the 31 minmax scaled variables, scaled based on the S3000 samples      \n",
    "    # since we stripped the prefix V31. from the column names, need to use both in the loop\n",
    "    for old_col, new_col in zip(V_columns, nV_columns):\n",
    "        row['v31_minmax'][new_col]=df_H1_scaled.iloc[i][old_col]\n",
    "    ### the clustering result and distance to center with ranking\n",
    "    for col in K_columns:\n",
    "        row['cluster_v31_minmax'][col]=df_H1.iloc[i][col]  \n",
    "    if i<= 2: print(row)\n",
    "    coll_res_H1.insert_one(row)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Classification:</b> A three step approach to determine and train a classification model to separate the clusters.  This is just a test run.  Currently, no analysis is based on this classification.\n",
    "<ul>\n",
    "<li>Based on the 3000 samples in H1.</li>\n",
    "<li>Use the clustering result (K=k) on v31_minmax on the 3000 samples as the y variables.</li>\n",
    "<li>Train a SVC classifier to classify the clusters.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Use GridSearch to determine the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "{'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "## Set df_scaled as X and df_res[\"K=5'\"] as y\n",
    "X = df_H1_scaled\n",
    "y = df_res[\"K=5\"]\n",
    "## Grid search for the best hyperparameters for SVC\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['linear', 'rbf']}\n",
    "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=3, n_jobs=-1)\n",
    "grid.fit(X, y)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Use a ten-fold cross-validation to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy from the 10 trials: 0.984  with standard deviation: 0.010\n"
     ]
    }
   ],
   "source": [
    "## Test the best hyperparameters using cross-validation\n",
    "n_folds=10\n",
    "model = SVC(**grid.best_params_)\n",
    "cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print(f\"Average Accuracy from the {n_folds} trials: {scores.mean():.3f}  with standard deviation: {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Fit the model with the best hyperparameters and save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n",
      "Accuracy: 0.996\n",
      "[[1056    1    0    3    1]\n",
      " [   2  832    0    0    3]\n",
      " [   0    0   58    0    0]\n",
      " [   0    1    0  813    0]\n",
      " [   0    0    0    1  229]]\n"
     ]
    }
   ],
   "source": [
    "## Use the best hyperparameters to fit SVC and save the trained model\n",
    "model.fit(X, y)\n",
    "## Save the trained model\n",
    "joblib.dump(model, './/trained_models//SVC_H1_S3000_v31_minmax.joblib')\n",
    "print(\"Model saved\")\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "print(f\"Accuracy: {accuracy_score(y, y_pred):.3f}\")\n",
    "print(confusion_matrix(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.996\n"
     ]
    }
   ],
   "source": [
    "## Retrive the saved model and use it to classify the data\n",
    "model = joblib.load('.//trained_models//SVC_H1_S3000_v31_minmax.joblib')\n",
    "y_pred = model.predict(X)\n",
    "print(f\"Accuracy: {accuracy_score(y, y_pred):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
