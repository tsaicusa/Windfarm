{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Working on the H1 Data</b>: The file contains some analysis on the 3000 samples from H1 (Windfarm 1).\n",
    "Specifically, it \n",
    "<ul>\n",
    "<li>Retrieve the H1 data from the mongoDB collection S6000_Data.</li>\n",
    "<li>Minmax scale the 3000 samples on the 31 feature variables.</li>\n",
    "<li>Cluster the 3000 samples into clusters with K=3, 4, 5, 6 using the v31_minmax variables.  Save the clustering models.</li>\n",
    "<li>Apply the SVC classification model on clusers with K=5 and save the classification model.</li>\n",
    "</ul>\n",
    "\n",
    "<em>!!! This program does not add, delete or update any collection in the database Windfarm_S6000.</em>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on converting Jupyter Notebook output to MS Word Documents\n",
    "\n",
    "- The best way to convert the ipynb file (Jupyter Notebook) to a docx file is to follow the two step approach explained in: https://blog.ouseful.info/2017/06/13/using-jupyter-notebooks-for-assessment-export-as-word-docx-extension/, i.e. run the following two commands on the anaconda command line\n",
    "- Step 1: $ jupyter nbconvert --no-input --to html file_name.ipynb (use --no-input to exclude code cells, i.e. convert only markdown cells)\n",
    "\n",
    "- Step 2: $ pandoc -s file_name.html -o file_name.docx\n",
    "- This approach produces, by far, the best quality docx ouput, no distortion of either the text or the graphs.  The only drawback is that it include the hidden code cells made by the \"hide all\" extension.  I have to manually delete those contents from the produced docx file.\n",
    "#### convert ijynb to a docx file\n",
    "- https://nbconvert.readthedocs.io/en/latest/index.html\n",
    "- install nbconvert [pip install nbconvert]\n",
    "- install pandoc: [https://github.com/jgm/pandoc/releases/tag/3.1.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "oneDrive_root={}\n",
    "oneDrive_root[1]=\"C:\\\\Users\\\\Chihyang\\OneDrive for Business\\\\\"\n",
    "oneDrive_root[2]=\"C:\\\\Users\\\\Chihyang\\OneDrive for Business\\\\\"\n",
    "oneDrive_root[3]=\"C:\\\\Users\\\\tsaic\\\\OneDrive - State University of New York at New Paltz\\\\\"  # laptop\n",
    "\n",
    "site=3   # the short or long business OneDrive directory name\n",
    "lib_dir=oneDrive_root[site]+'\\\\Prudentia\\\\Tsaipy'\n",
    "# append additional library path for this study\n",
    "sys.path.append(lib_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import sklearn as sk\n",
    "#from sklearn.cluster import KMeans\n",
    "from dbconnect.mongodb import cursor_to_dataframe3\n",
    "from WFN_lib.WindFarm import distance2center,  write_df_sub2Excel\n",
    "from WFN_lib.mongodb_util import flatten_dictionary\n",
    "from WFN_lib.cluster_classify import cluster_analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "from dbconnect import mongodb as mdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>MongoDb Parameters:</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in coll_data = 6000\n"
     ]
    }
   ],
   "source": [
    "## pymongo connect to mongodb database and collection\n",
    "db_name = \"Windfarm_S6000\"\n",
    "client = MongoClient('localhost', 27017)  # connect to the db engine\n",
    "db = client[db_name]\n",
    "coll_data=db['S6000_Data']\n",
    "\n",
    "print(f\"Rows in coll_data = {coll_data.count_documents({})}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>I. Clustering the 3000 cases in H1</b>\n",
    "\n",
    "<b>DO NOT</b> use <b>v31_minmax</b> obtained from S6000 since it is scaled with the whole 6000 cases.  <b>For validation purpose</b> with H2 files, we need to scale v31 with only the 3000 cases in H1.\n",
    "\n",
    "Thus, for the S3000 database, we only store v31_minmax variables scaled based on the 3000 files in H1.  If you need variable values in the original scale, just retrieve them from collection <b>Data_S6000</b> with the indices H1_xxxx, where xxxx ranges from <b>H1_0001</b> to <b>H1_3000</b>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data from H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 31)\n",
      "         v31.spectralCentroid  v31.spectralCrest  v31.spectralDecrease  \\\n",
      "H1_0001             63.081251          28.079092            -92.466205   \n",
      "H1_0002             57.650009          30.680069           -259.422066   \n",
      "H1_0003             75.049887          25.424498            -22.843544   \n",
      "\n",
      "         v31.spectralEntropy  v31.spectralFlatness  v31.spectralFlux  \\\n",
      "H1_0001             0.143241              0.010610      1.087143e-07   \n",
      "H1_0002             0.067829              0.011763      4.547078e-05   \n",
      "H1_0003             0.217764              0.008283      3.236673e-06   \n",
      "\n",
      "         v31.spectralKurtosis  v31.spectralRolloffPoint  v31.spectralSkewness  \\\n",
      "H1_0001           1420.161272                115.968033             25.345770   \n",
      "H1_0002            847.458521                 72.747519             22.280157   \n",
      "H1_0003            180.938195                177.970026              9.196154   \n",
      "\n",
      "         v31.spectralSlope  ...      v31.PR  v31.Fo  v31.AMfactor   v31.DAM  \\\n",
      "H1_0001      -1.373689e-11  ...   69.138538     0.9      0.239249  1.339453   \n",
      "H1_0002      -2.505266e-09  ...    7.843309     0.8      0.111174  1.217609   \n",
      "H1_0003      -2.185107e-10  ...  104.108914     0.7      0.827716  7.589734   \n",
      "\n",
      "         v31.peakloc_unweightedSPL   v31.L63  v31.L125  v31.L250  v31.L500  \\\n",
      "H1_0001                        0.2  3.403435  2.685078  3.967624  1.748563   \n",
      "H1_0002                        0.5  6.255368  6.074805  3.424099  2.094934   \n",
      "H1_0003                        2.8  7.478385  6.192284  6.185724  5.644156   \n",
      "\n",
      "         v31.L1000  \n",
      "H1_0001   1.427153  \n",
      "H1_0002   1.888449  \n",
      "H1_0003  13.776354  \n",
      "\n",
      "[3 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "res = coll_data.find({'site':'H1'},{'v31':1})  # index '_id' is automatically added\n",
    "df_H1 = cursor_to_dataframe3(res,\"_id\")\n",
    "print(df_H1.shape)\n",
    "print(df_H1.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use the unscaled data and ask the function to scale it based on the 3000 files\n",
    "## df_res is the dataframe with the cluster allotment; df_scaled is the scaled data\n",
    "## Save the trained KMeans model to the directory save_model_dir\n",
    "df_res, df_H1_scaled=cluster_analysis(df_H1, [3,4,5,6], random_state=42, scaler_type=\"MinMax\",n_init=10, max_iter=300, tol=0.0001,\n",
    "                                   save_model_dir='.//trained_models//clustering//', model_file_name='Kmeans_H1_S3000_v31_MinMax_K=', \n",
    "                                   scaler_model_dir='.//trained_models//scalers//', scaler_model_name='_scaler_v31_H1_S3000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster center of the first two clusters:\n",
      " [[0.02119858 0.81810136 0.99622386 0.22347605 0.02835462 0.100438\n",
      "  0.00387291 0.02865809 0.05283666 0.90645383 0.06808207 0.99802304\n",
      "  0.02119692 0.49835447 0.37158984 0.24629471 0.46649812 0.56352498\n",
      "  0.01013088 0.06774282 0.92194446 0.01180246 0.73023353 0.05695455\n",
      "  0.03825488 0.12299717 0.12356561 0.08505314 0.05038632 0.04593872\n",
      "  0.06330229]\n",
      " [0.02001092 0.8734187  0.98782653 0.17282922 0.04541925 0.26940534\n",
      "  0.00797646 0.0247377  0.06823995 0.70452366 0.07394631 0.99487267\n",
      "  0.01294426 0.57682307 0.31523801 0.17429152 0.36345346 0.08566308\n",
      "  0.00397009 0.03529413 0.96251697 0.00525148 0.72414709 0.02542821\n",
      "  0.02344513 0.08106072 0.12233221 0.09032051 0.04726544 0.03935171\n",
      "  0.05459326]]\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "file1=joblib.load('.//trained_models//clustering//Kmeans_H1_S3000_v31_MinMax_K=5.joblib')\n",
    "print(\"cluster center of the first two clusters:\\n\",file1.cluster_centers_[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=5\n",
      "[0 1 2 3 4]\n",
      "(1061, 12)\n",
      "Sort rows by:  rank_K=5\n",
      "cluster = 0:  Shape of dk: (1061, 12)\n",
      "         K=3  K=4  K=5  K=6  dist_K=3  rank_K=3  dist_K=4  rank_K=4  dist_K=5  \\\n",
      "H1_0307    1    2    0    3  0.321323       485  0.111703         1  0.109369   \n",
      "H1_2459    1    2    0    3  0.222810       107  0.161323        28  0.121509   \n",
      "H1_1010    1    2    0    3  0.280208       303  0.117239         2  0.122694   \n",
      "\n",
      "         rank_K=5  dist_K=6  rank_K=6  \n",
      "H1_0307         1  0.123694         6  \n",
      "H1_2459         2  0.113775         1  \n",
      "H1_1010         3  0.120491         2  \n",
      "(837, 12)\n",
      "Sort rows by:  rank_K=5\n",
      "cluster = 1:  Shape of dk: (837, 12)\n",
      "         K=3  K=4  K=5  K=6  dist_K=3  rank_K=3  dist_K=4  rank_K=4  dist_K=5  \\\n",
      "H1_1954    2    1    1    1  0.157574         3  0.129302         1  0.127962   \n",
      "H1_2346    2    1    1    1  0.182414         6  0.145103         2  0.141174   \n",
      "H1_1493    2    1    1    1  0.149650         1  0.145251         3  0.145966   \n",
      "\n",
      "         rank_K=5  dist_K=6  rank_K=6  \n",
      "H1_1954         1  0.261256        61  \n",
      "H1_2346         2  0.248391        45  \n",
      "H1_1493         3  0.307513       132  \n",
      "(58, 12)\n",
      "Sort rows by:  rank_K=5\n",
      "cluster = 2:  Shape of dk: (58, 12)\n",
      "         K=3  K=4  K=5  K=6  dist_K=3  rank_K=3  dist_K=4  rank_K=4  dist_K=5  \\\n",
      "H1_2476    0    2    2    4  1.103428       270  1.136899      1025  0.346939   \n",
      "H1_1756    0    2    2    4  1.278990       288  1.396471      1044  0.355749   \n",
      "H1_2927    0    2    2    4  1.282423       291  1.345360      1038  0.361462   \n",
      "\n",
      "         rank_K=5  dist_K=6  rank_K=6  \n",
      "H1_2476         1  0.342250         1  \n",
      "H1_1756         2  0.342816         2  \n",
      "H1_2927         3  0.347644         3  \n",
      "(814, 12)\n",
      "Sort rows by:  rank_K=5\n",
      "cluster = 3:  Shape of dk: (814, 12)\n",
      "         K=3  K=4  K=5  K=6  dist_K=3  rank_K=3  dist_K=4  rank_K=4  dist_K=5  \\\n",
      "H1_0859    1    0    3    0  0.275600       281  0.127391         1  0.126778   \n",
      "H1_2212    1    0    3    0  0.314682       447  0.153928         4  0.150613   \n",
      "H1_0764    1    0    3    0  0.208822        75  0.152616         2  0.155310   \n",
      "\n",
      "         rank_K=5  dist_K=6  rank_K=6  \n",
      "H1_0859         1  0.153999         6  \n",
      "H1_2212         2  0.163856        10  \n",
      "H1_0764         3  0.154985         7  \n",
      "(230, 12)\n",
      "Sort rows by:  rank_K=5\n",
      "cluster = 4:  Shape of dk: (230, 12)\n",
      "         K=3  K=4  K=5  K=6  dist_K=3  rank_K=3  dist_K=4  rank_K=4  dist_K=5  \\\n",
      "H1_2058    0    3    4    2  0.408581        17  0.236833         2  0.227650   \n",
      "H1_2812    0    3    4    2  0.358021         2  0.235990         1  0.231090   \n",
      "H1_1637    0    3    4    2  0.462640        34  0.336434         4  0.325272   \n",
      "\n",
      "         rank_K=5  dist_K=6  rank_K=6  \n",
      "H1_2058         1  0.237122         2  \n",
      "H1_2812         2  0.232038         1  \n",
      "H1_1637         3  0.330431         3  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xlsxwriter\\workbook.py:339: UserWarning: Calling close() on already closed file.\n",
      "  warn(\"Calling close() on already closed file.\")\n"
     ]
    }
   ],
   "source": [
    "df_H1_clusters, cluster_size_H1, cluster_means_H1, cluster_vars_H1 = distance2center(df_H1_scaled, df_res, [3,4,5,6])\n",
    "write_df_sub2Excel(df_H1_clusters, method='', n_clusters=5, filename='.//Results_H1//S3000_Results//Cluster_dist2center_H1_S3000_v31_minmax.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Write the K=5 Clustering result with additional details into an Excel file:</b> Data in a cluster is presented in one Worksheet of the file.   \n",
    "\n",
    "In each cluster, rows are sorted from the nearest to its class center the farthest.  For each row (case), columns include \n",
    "<ol>\n",
    "<li> Cluster it is assigned to,</li>\n",
    "<li>distance to cluster center,</li>\n",
    "<li>ranking of distance to center,</li>\n",
    "<li>two IOA ratings (prominenace and modulation depth) for each band (four bands), and</li> \n",
    "<li>YAMnet top five classes with corresponding probabilities (10 columns)</li>\n",
    "</ol>\n",
    "\n",
    "<b>Cluster Characteristics</b>:\n",
    "<ul>\n",
    "<li>Cluster 0: 1061 cases; Low confidence even to the first YAMnet class,</li>\n",
    "<li>Cluster 1: 837 cases; Low confidence even to the first YAMnet class,</li>\n",
    "<li>Cluster 2: 58 cases; High confidence to the first YAMnet class; first class has lots of \"silence\"</li>\n",
    "<li>Cluster 3: 814 cases; Moderate confidence to the first YAMnet class; first class has lots of \"silence\", and</li> \n",
    "<li>Cluster 4: 230 cases; Moderate confidence to the first YAMnet class; first class has lots of \"Animal\"</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1061\n",
      "1     837\n",
      "2      58\n",
      "3     814\n",
      "4     230\n",
      "Name: K=5, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cnt=df_res['K=5'].value_counts()\n",
    "cnt.sort_index(inplace=True)\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Classification:</b> A three step approach to determine and train a classification model to separate the clusters.  This is just a test run.  Currently, no analysis is based on this classification.\n",
    "<ul>\n",
    "<li>Based on the 3000 samples in H1.</li>\n",
    "<li>Use the clustering result (K=k) on v31_minmax on the 3000 samples as the y variables.</li>\n",
    "<li>Train a SVC classifier to classify the clusters.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Use GridSearch to determine the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "## Set df_scaled as X and df_res[\"K=5'\"] as y\n",
    "X = df_H1_scaled\n",
    "y = df_res[\"K=5\"]\n",
    "## Grid search for the best hyperparameters for SVC\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['linear', 'rbf']}\n",
    "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=3, n_jobs=-1)\n",
    "grid.fit(X, y)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Use a ten-fold cross-validation to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy from the 10 trials: 0.984  with standard deviation: 0.010\n"
     ]
    }
   ],
   "source": [
    "## Test the best hyperparameters using cross-validation\n",
    "n_folds=10\n",
    "model = SVC(**grid.best_params_)\n",
    "cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print(f\"Average Accuracy from the {n_folds} trials: {scores.mean():.3f}  with standard deviation: {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Fit the model with the best hyperparameters and save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n",
      "Accuracy: 0.996\n",
      "[[1056    1    0    3    1]\n",
      " [   2  832    0    0    3]\n",
      " [   0    0   58    0    0]\n",
      " [   0    1    0  813    0]\n",
      " [   0    0    0    1  229]]\n"
     ]
    }
   ],
   "source": [
    "## Use the best hyperparameters to fit SVC and save the trained model\n",
    "model.fit(X, y)\n",
    "## Save the trained model\n",
    "joblib.dump(model, './/trained_models//classification//SVC_H1_S3000_v31_minmax_K=5.joblib')\n",
    "print(\"Model saved\")\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "print(f\"Accuracy: {accuracy_score(y, y_pred):.3f}\")\n",
    "print(confusion_matrix(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.996\n"
     ]
    }
   ],
   "source": [
    "## Retrive the saved model and use it to classify the data\n",
    "model = joblib.load('.//trained_models//classification//SVC_H1_S3000_v31_minmax_K=5.joblib')\n",
    "y_pred = model.predict(X)\n",
    "print(f\"Accuracy: {accuracy_score(y, y_pred):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
